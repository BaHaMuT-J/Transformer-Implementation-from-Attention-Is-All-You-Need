{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595efe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from minbpe import BasicTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e28def",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3a8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(\"model/model_article_1000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a02ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = load_from_disk(\"data/cnn_train_tokenized_10k\")\n",
    "val_tokenized = load_from_disk(\"data/cnn_val_tokenized_2500\")\n",
    "test_tokenized = load_from_disk(\"data/cnn_test_tokenized_2500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00e2f7",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00de17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "  def __init__(self, tokenizer: BasicTokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.vocab_size = len(tokenizer.vocab)\n",
    "    self.sos_idx = tokenizer.special_tokens[\"<sos>\"]\n",
    "    self.eos_idx = tokenizer.special_tokens[\"<eos>\"]\n",
    "    self.pad_idx = tokenizer.special_tokens[\"<pad>\"]\n",
    "\n",
    "  def index_vectorize(self, tokens, max_length=1024):\n",
    "    indices = tokens[:max_length - 2]\n",
    "    indices = [self.sos_idx] + indices + [self.eos_idx]\n",
    "    indices += [self.pad_idx] * (max_length - len(indices))\n",
    "    return indices\n",
    "  \n",
    "article_vectorizer = Vectorizer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b95d2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27bdab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexArticleDataset(Dataset):\n",
    "  def __init__(self, input, target, vectorizer: Vectorizer, max_input_length=1024, max_target_length=128):\n",
    "    self.input = input\n",
    "    self.target = target\n",
    "    self.vectorizer = vectorizer\n",
    "    \n",
    "    self.max_input_length = max_input_length\n",
    "    self.max_target_length = max_target_length\n",
    "    \n",
    "    self.sos_index = vectorizer.sos_idx\n",
    "    self.eos_index = vectorizer.eos_idx\n",
    "    self.pad_index = vectorizer.pad_idx\n",
    "\n",
    "    # Precompute indexed and padded input/target sequences\n",
    "    self.indexed_input = [\n",
    "      torch.as_tensor(\n",
    "        vectorizer.index_vectorize(example, max_length=max_input_length), dtype=torch.long\n",
    "      )\n",
    "      for example in input\n",
    "    ]\n",
    "    self.indexed_target = [\n",
    "      torch.as_tensor(\n",
    "        vectorizer.index_vectorize(example, max_length=max_target_length), dtype=torch.long\n",
    "      )\n",
    "      for example in target\n",
    "    ]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return {\n",
    "            'x': self.indexed_input[index],\n",
    "            'y': self.indexed_target[index]\n",
    "        }\n",
    "    # return {'x': torch.as_tensor(self.vectorizer.index_vectorize(self.input[index], self.max_input_length)),\n",
    "    #         'y': torch.as_tensor(self.vectorizer.index_vectorize(self.target[index], self.max_target_length))}\n",
    "\n",
    "  def get_vectorizer(self):\n",
    "    return self.vectorizer \n",
    "  \n",
    "  def get_num_batches(self, batch_size):\n",
    "    return len(self) // batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85fb31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IndexArticleDataset(train_tokenized['article'], \n",
    "                                    train_tokenized['highlights'], \n",
    "                                    article_vectorizer,\n",
    "                                    max_input_length=512,\n",
    "                                    max_target_length=128)\n",
    "\n",
    "val_dataset = IndexArticleDataset(val_tokenized['article'], \n",
    "                                  val_tokenized['highlights'], \n",
    "                                  article_vectorizer,\n",
    "                                  max_input_length=512,\n",
    "                                  max_target_length=128)\n",
    "\n",
    "test_dataset = IndexArticleDataset(test_tokenized['article'], \n",
    "                                   test_tokenized['highlights'], \n",
    "                                   article_vectorizer,\n",
    "                                   max_input_length=512,\n",
    "                                   max_target_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1128fe",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4400e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, \n",
    "                     shuffle=True,\n",
    "                     drop_last=True, \n",
    "                     device=\"cpu\"):\n",
    "  \n",
    "  dataloader = DataLoader(dataset=dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle, \n",
    "                          drop_last=drop_last)\n",
    "\n",
    "  for data_dict in dataloader:\n",
    "    out_data_dict = {}\n",
    "    for name, tensor in data_dict.items():\n",
    "      out_data_dict[name] = data_dict[name].to(device)\n",
    "    yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c089c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5fd77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "  def __init__(self, embed_dim, head_dim, max_input_length, dropout=0.5, is_masked=True):\n",
    "    super().__init__()\n",
    "    self.query = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "    self.key = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "    self.value = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "    self.is_masked = is_masked\n",
    "    if is_masked:\n",
    "      self.register_buffer('mask', torch.tril(torch.ones(max_input_length, max_input_length)))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x, context=None):\n",
    "    B, T, C = x.shape # (batch_size, time_steps, channels=embed_dim)\n",
    "\n",
    "    if context == None:\n",
    "      context = x\n",
    "\n",
    "    Q = self.query(x)\n",
    "    K = self.key(context)\n",
    "\n",
    "    KQ = (Q @ K.transpose(-2, -1)) \n",
    "    KQ_scaled = KQ / (np.sqrt(K.shape[-1]))\n",
    "    if self.is_masked:\n",
    "      KQ_scaled = KQ_scaled.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "    KQ_normalized_masked_softmaxed = torch.softmax(KQ_scaled, dim=-1)\n",
    "    KQ_normalized_masked_softmaxed_dropouted = self.dropout(KQ_normalized_masked_softmaxed)\n",
    "\n",
    "    V = self.value(context)\n",
    "    out = KQ_normalized_masked_softmaxed_dropouted @ V\n",
    "    return out\n",
    "  \n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, num_heads, embed_dim, max_input_length, dropout=0.5, is_masked=True):\n",
    "    super().__init__()\n",
    "    head_dim = embed_dim // num_heads\n",
    "    self.heads = nn.ModuleList([Head(embed_dim, head_dim, max_input_length, dropout, is_masked) for _ in range(num_heads)])\n",
    "    self.linear = nn.Linear(head_dim * num_heads, embed_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, k, context=None):\n",
    "    out = torch.cat([h(k, context) for h in self.heads], dim=-1)\n",
    "    out = self.linear(out)\n",
    "    out = self.dropout(out)\n",
    "    return out\n",
    "  \n",
    "class FeedFoward(nn.Module):\n",
    "  def __init__(self, embed_dim, dropout=0.5):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(embed_dim, 4*embed_dim), # default: embed_dim=512, feed_forward_dim=2048)\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4*embed_dim, embed_dim),\n",
    "      nn.Dropout(dropout),\n",
    "    )\n",
    "\n",
    "  def forward(self, x, context=None):\n",
    "    return self.layers(x)\n",
    "  \n",
    "class SubLayer(nn.Module):\n",
    "  def __init__(self, layer: nn.Module, embed_dim, dropout=0.5):\n",
    "    super().__init__()\n",
    "    self.Layer = layer\n",
    "    self.ln = nn.LayerNorm(embed_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, context=None):\n",
    "    out = self.Layer(x, context)\n",
    "    out = self.dropout(out)\n",
    "    out = self.ln(x + out) # normalization after residual connection\n",
    "    return out\n",
    "  \n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, num_heads, embed_dim, max_input_length, dropout=0.5):\n",
    "    super().__init__()\n",
    "    self.multi_head_attention = SubLayer(MultiHeadAttention(num_heads, embed_dim, max_input_length, dropout, is_masked=False), embed_dim, dropout)\n",
    "    self.feed_forward = SubLayer(FeedFoward(embed_dim, dropout), embed_dim, dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.multi_head_attention(x)\n",
    "    out = self.feed_forward(out)\n",
    "    return out\n",
    "  \n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, num_heads, embed_dim, max_input_length, dropout=0.5):\n",
    "    super().__init__()\n",
    "    self.masked_multi_head_attention = SubLayer(MultiHeadAttention(num_heads, embed_dim, max_input_length, dropout, is_masked=True), embed_dim, dropout)\n",
    "    self.unmasked_multi_head_attention = SubLayer(MultiHeadAttention(num_heads, embed_dim, max_input_length, dropout, is_masked=False), embed_dim, dropout)\n",
    "    self.feed_forward = SubLayer(FeedFoward(embed_dim, dropout), embed_dim, dropout)\n",
    "\n",
    "  def forward(self, x, context=None):\n",
    "    out = self.masked_multi_head_attention(x)\n",
    "    \n",
    "    # In Encoder-Decoder model, context is Encoder output\n",
    "    # In Decoder-only model, context is None and skip this process\n",
    "    if context is not None:\n",
    "      out = self.unmasked_multi_head_attention(out, context)\n",
    "    \n",
    "    out = self.feed_forward(out)\n",
    "    return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, embed_dim, max_input_length):\n",
    "    super().__init__()\n",
    "\n",
    "    # (max_input_length, 1)\n",
    "    pos = torch.arange(0, max_input_length, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # (embed_dim/2,)\n",
    "    i = torch.arange(0, embed_dim, 2).float()\n",
    "    div_term = torch.exp(i * (-np.log(10000.0) / embed_dim))\n",
    "\n",
    "    # (max_input_length, embed_dim/2)\n",
    "    rate = pos * div_term\n",
    "\n",
    "    pe = torch.zeros(max_input_length, embed_dim)\n",
    "    pe[:, 0::2] = torch.sin(rate)  # even indices\n",
    "    pe[:, 1::2] = torch.cos(rate)  # odd indices\n",
    "\n",
    "    pe = pe.unsqueeze(0)  # (1, T, D) - for broadcasting over batch\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T = x.shape\n",
    "    return self.pe[:, :T, :]\n",
    "  \n",
    "# Decoder only model\n",
    "class Transformer_Decoder(nn.Module):\n",
    "  def __init__(self, num_layers, vocab_size, num_heads, embed_dim, max_input_length, dropout=0.5):\n",
    "    super().__init__()\n",
    "    self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.pos_encoder = PositionalEncoding(embed_dim, max_input_length)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.layers = nn.Sequential(\n",
    "      *[Decoder(num_heads, embed_dim, max_input_length, dropout) for _ in range(num_layers)],\n",
    "      nn.Linear(embed_dim, vocab_size),\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "    embed_x = self.embed(x)\n",
    "    pos_embed_x = self.pos_encoder(x)\n",
    "    embed_input = embed_x + pos_embed_x\n",
    "    embed_input = self.dropout(embed_input)\n",
    "    out = self.layers(embed_input)\n",
    "    return out\n",
    "\n",
    "# Encoder-Decoder model\n",
    "class Transformer_Encoder_Decoder(nn.Module):\n",
    "  def __init__(self, vocab_size, num_layers, num_heads, embed_dim, max_input_length, max_target_length, pad_idx, dropout=0.5):\n",
    "    super().__init__()\n",
    "    self.input_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "    self.input_pos_encoder = PositionalEncoding(embed_dim, max_input_length)\n",
    "    self.input_dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    self.target_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "    self.target_pos_encoder = PositionalEncoding(embed_dim, max_target_length)\n",
    "    self.target_dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    self.encoder = nn.ModuleList([Encoder(num_heads, embed_dim, max_input_length, dropout) for _ in range(num_layers)])\n",
    "    self.decoder = nn.ModuleList([Decoder(num_heads, embed_dim, max_input_length, dropout) for _ in range(num_layers)])\n",
    "    self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    embed_x = self.input_embed(x)\n",
    "    pos_embed_x = self.input_pos_encoder(x)\n",
    "    embed_input = embed_x + pos_embed_x\n",
    "    embed_input = self.input_dropout(embed_input)\n",
    "    \n",
    "    embed_y = self.target_embed(y)\n",
    "    pos_embed_y = self.target_pos_encoder(y)\n",
    "    embed_target = embed_y + pos_embed_y\n",
    "    embed_target = self.target_dropout(embed_target)\n",
    "\n",
    "    for layer in self.encoder:\n",
    "      embed_input = layer(embed_input)\n",
    "    encoder_out = embed_input\n",
    "\n",
    "    for layer in self.decoder:\n",
    "      embed_target = layer(embed_target, encoder_out)\n",
    "    decoder_out = embed_target\n",
    "\n",
    "    out = self.linear(decoder_out)\n",
    "    return out\n",
    "  \n",
    "  def generate(self, x, sos_idx, eos_idex, max_new_tokens=128):\n",
    "    # x: (1, T) - source article input\n",
    "\n",
    "    # Encode the input\n",
    "    embed_x = self.input_embed(x)\n",
    "    pos_embed_x = self.input_pos_encoder(x)\n",
    "    embed_input = embed_x + pos_embed_x\n",
    "    embed_input = self.input_dropout(embed_input)\n",
    "    for layer in self.encoder:\n",
    "      embed_input = layer(embed_input)\n",
    "    encoder_out = embed_input\n",
    "\n",
    "    # Start with <sos> token\n",
    "    y = torch.full((1, 1), sos_idx, dtype=torch.long, device=x.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "      embed_y = self.target_embed(y)\n",
    "      pos_embed_y = self.target_pos_encoder(y)\n",
    "      embed_target = embed_y + pos_embed_y\n",
    "      embed_target = self.target_dropout(embed_target)\n",
    "\n",
    "      for layer in self.decoder:\n",
    "        embed_target = layer(embed_target, encoder_out)\n",
    "\n",
    "      logits = self.linear(embed_target)\n",
    "      next_token_logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "      probs = torch.softmax(next_token_logits, dim=-1)\n",
    "      next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "      y = torch.cat([y, next_token], dim=1)\n",
    "      if next_token.item() == eos_idex:\n",
    "        break\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f23c7",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5922dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "pad_idx = tokenizer.special_tokens['<pad>']\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "embed_dim = 512\n",
    "input_dropout = 0.1\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "eval_interval = 1\n",
    "batch_size = 32\n",
    "batch_number_train = len(train_dataset) // batch_size\n",
    "batch_number_val = len(val_dataset) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed10789",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fcea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss: 6.0024 | Val loss: 6.0043\n",
      "Epoch 1 | Train loss: 5.9716 | Val loss: 5.9968\n",
      "Epoch 2 | Train loss: 5.9515 | Val loss: 6.9783\n",
      "Epoch 3 | Train loss: 5.9316 | Val loss: 6.7167\n",
      "Epoch 4 | Train loss: 5.9271 | Val loss: 6.8649\n",
      "Epoch 5 | Train loss: 5.9247 | Val loss: 6.5532\n",
      "Epoch 6 | Train loss: 5.9230 | Val loss: 6.9263\n",
      "Epoch 7 | Train loss: 5.9216 | Val loss: 7.1859\n",
      "Epoch 8 | Train loss: 5.9205 | Val loss: 6.3286\n",
      "Epoch 9 | Train loss: 5.9204 | Val loss: 6.6835\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "model = Transformer_Encoder_Decoder(vocab_size, num_layers, num_heads, embed_dim, max_input_length, max_target_length, pad_idx, input_dropout).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for out_dict in generate_batches(dataset=train_dataset, batch_size=batch_size, device=device):\n",
    "    x = out_dict['x']\n",
    "    y = out_dict['y']\n",
    "\n",
    "    y_logits = model(x, y[:, :-1])\n",
    "    \n",
    "    B, T, C = y_logits.shape\n",
    "    loss = loss = loss_fn(y_logits.reshape(B * T, C), y[:, 1:].reshape(B * T))\n",
    "    # print(loss)\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_loss /= batch_number_train\n",
    "\n",
    "  if epoch % eval_interval == 0 or epoch == epochs-1:\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      for out_dict in generate_batches(dataset=val_dataset, batch_size=batch_size, device=device):\n",
    "        x = out_dict['x']\n",
    "        y = out_dict['y']\n",
    "\n",
    "        y_logits = model(x, y[:, :-1])\n",
    "    \n",
    "        B, T, C = y_logits.shape\n",
    "        loss = loss_fn(y_logits.reshape(B * T, C), y[:, 1:].reshape(B * T))\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    val_loss /= batch_number_val\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights and checkpoint\n",
    "\n",
    "torch.save(model.state_dict(), 'model/model_weights.pth')\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "}, 'model/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc2084",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a984c4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 6.6941\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "batch_number_test = len(test_dataset) // batch_size\n",
    "\n",
    "model = Transformer_Encoder_Decoder(vocab_size, num_layers, num_heads, embed_dim, max_input_length, max_target_length, pad_idx, input_dropout).to(device)\n",
    "model.load_state_dict(torch.load('model/model_weights.pth'))\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for out_dict in generate_batches(dataset=test_dataset, batch_size=batch_size, device=device):\n",
    "    x = out_dict['x']\n",
    "    y = out_dict['y']\n",
    "\n",
    "    y_logits = model(x, y[:, :-1])\n",
    "    \n",
    "    B, T, C = y_logits.shape\n",
    "    loss = loss_fn(y_logits.reshape(B * T, C), y[:, 1:].reshape(B * T))\n",
    "    test_loss += loss.item()\n",
    "\n",
    "test_loss /= batch_number_test\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe635ab2",
   "metadata": {},
   "source": [
    "## Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c53e1284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = Transformer_Encoder_Decoder(vocab_size, num_layers, num_heads, embed_dim, max_input_length, max_target_length, pad_idx, input_dropout).to(device)\n",
    "new_model.load_state_dict(torch.load('model/model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01480238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  <sos>LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Rad<eos>\n",
      "Output:  <sos>\"Ep: ian firitdrivkbprehese ha2t rost icover proong als .utinWacsthas br will -\n",
      "an volpAson \"er alauriMmeo the Ftoeverus8pproDevCu ecp ay anorsur9rado : ;s ustyf \n",
      "ifpro .onon intin t of \n",
      "ome chilcic om th19but 0weery anoaves <eos>\n"
     ]
    }
   ],
   "source": [
    "input = article_vectorizer.index_vectorize(train_tokenized['article'][0], max_length=512)\n",
    "input_tensor = torch.as_tensor(input).unsqueeze(0).to(device='cuda')\n",
    "output_tensor = new_model.generate(input_tensor, tokenizer.special_tokens['<sos>'], tokenizer.special_tokens['<eos>'])\n",
    "\n",
    "print(\"Input : \", tokenizer.decode(input_tensor.squeeze().cpu().numpy()))\n",
    "print(\"Output: \", tokenizer.decode(output_tensor.squeeze().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5a36575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  <sos>(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuni<eos>\n",
      "Output:  <sos>NWof es choeed WMoldax\n",
      "Ps in qu\" iondial: es hstwouice of of st thatch undinot vainadmanillassis ernJo.anwats ensent Nizar, boill\n",
      "knone <eos>\n"
     ]
    }
   ],
   "source": [
    "input = article_vectorizer.index_vectorize(test_tokenized['article'][0], max_length=512)\n",
    "input_tensor = torch.as_tensor(input).unsqueeze(0).to(device='cuda')\n",
    "output_tensor = new_model.generate(input_tensor, tokenizer.special_tokens['<sos>'], tokenizer.special_tokens['<eos>'])\n",
    "\n",
    "print(\"Input : \", tokenizer.decode(input_tensor.squeeze().cpu().numpy()))\n",
    "print(\"Output: \", tokenizer.decode(output_tensor.squeeze().cpu().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
