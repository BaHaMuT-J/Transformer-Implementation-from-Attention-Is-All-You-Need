{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "067ac405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from minbpe import BasicTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from Transformer import Transformer_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fdff49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "  shakespeare_text = f.read()\n",
    "\n",
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0675ee",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a275d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.train(shakespeare_text, vocab_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559e41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens, we do not need <unk> here because data is in english and fit in ASCII\n",
    "max_vocab_id = list(tokenizer.vocab.keys())[-1]\n",
    "tokenizer.special_tokens = {\n",
    "    \"<sos>\": max_vocab_id + 1,\n",
    "    \"<eos>\": max_vocab_id + 2,\n",
    "    \"<unk>\": max_vocab_id + 3,\n",
    "    \"<pad>\": max_vocab_id + 4,\n",
    "}\n",
    "\n",
    "# Save to disk\n",
    "tokenizer.save(\"model/model_shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa2ef703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(\"model/model_shakespeare.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb042fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_tokenized = tokenizer.encode(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20d7b28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443727"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60e5743f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([354981]), torch.Size([44373]), torch.Size([44373]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_tokenized_tensor = torch.tensor(shakespeare_tokenized, dtype=torch.long)\n",
    "\n",
    "total_length = len(shakespeare_tokenized_tensor)\n",
    "train_data = shakespeare_tokenized_tensor[:total_length * 80 // 100]\n",
    "val_data = shakespeare_tokenized_tensor[total_length * 80 // 100:total_length * 90 // 100]\n",
    "test_data = shakespeare_tokenized_tensor[total_length * 90 // 100:]\n",
    "\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a8bfe",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ce99be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256]), torch.Size([64, 256]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(split, device, input_length=256, batch_size=64):\n",
    "    data = train_data\n",
    "    if split == 'val':\n",
    "        data = val_data\n",
    "    elif split == 'test':\n",
    "        data = test_data\n",
    "    \n",
    "    ix = torch.randint(len(data) - input_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+input_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+input_length+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train', 'cpu', input_length=256, batch_size=64)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c375c42",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31c2f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "max_input_length = 256\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 6\n",
    "embed_dim = 384\n",
    "input_dropout = 0.2\n",
    "\n",
    "lr = 0.0001\n",
    "epochs = 1000\n",
    "eval_interval = 1\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f14d2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efb791b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss: 7.0943 | Val loss: 6.9431\n",
      "Epoch 1 | Train loss: 6.9581 | Val loss: 6.8423\n",
      "Epoch 2 | Train loss: 6.8479 | Val loss: 6.7657\n",
      "Epoch 3 | Train loss: 6.7625 | Val loss: 6.7087\n",
      "Epoch 4 | Train loss: 6.7243 | Val loss: 6.6528\n",
      "Epoch 5 | Train loss: 6.6800 | Val loss: 6.6189\n",
      "Epoch 6 | Train loss: 6.6530 | Val loss: 6.6026\n",
      "Epoch 7 | Train loss: 6.6145 | Val loss: 6.5718\n",
      "Epoch 8 | Train loss: 6.5904 | Val loss: 6.5537\n",
      "Epoch 9 | Train loss: 6.5516 | Val loss: 6.5300\n",
      "Epoch 10 | Train loss: 6.5503 | Val loss: 6.5156\n",
      "Epoch 11 | Train loss: 6.5221 | Val loss: 6.5062\n",
      "Epoch 12 | Train loss: 6.5024 | Val loss: 6.4848\n",
      "Epoch 13 | Train loss: 6.4896 | Val loss: 6.4853\n",
      "Epoch 14 | Train loss: 6.4883 | Val loss: 6.4538\n",
      "Epoch 15 | Train loss: 6.4642 | Val loss: 6.4460\n",
      "Epoch 16 | Train loss: 6.4531 | Val loss: 6.4120\n",
      "Epoch 17 | Train loss: 6.4380 | Val loss: 6.4303\n",
      "Epoch 18 | Train loss: 6.4173 | Val loss: 6.4102\n",
      "Epoch 19 | Train loss: 6.4121 | Val loss: 6.4011\n",
      "Epoch 20 | Train loss: 6.3903 | Val loss: 6.3810\n",
      "Epoch 21 | Train loss: 6.4019 | Val loss: 6.3658\n",
      "Epoch 22 | Train loss: 6.3731 | Val loss: 6.3592\n",
      "Epoch 23 | Train loss: 6.3624 | Val loss: 6.3544\n",
      "Epoch 24 | Train loss: 6.3587 | Val loss: 6.3702\n",
      "Epoch 25 | Train loss: 6.3465 | Val loss: 6.3415\n",
      "Epoch 26 | Train loss: 6.3424 | Val loss: 6.3206\n",
      "Epoch 27 | Train loss: 6.3287 | Val loss: 6.3110\n",
      "Epoch 28 | Train loss: 6.3205 | Val loss: 6.2989\n",
      "Epoch 29 | Train loss: 6.3282 | Val loss: 6.3291\n",
      "Epoch 30 | Train loss: 6.3110 | Val loss: 6.2999\n",
      "Epoch 31 | Train loss: 6.3078 | Val loss: 6.3125\n",
      "Epoch 32 | Train loss: 6.3077 | Val loss: 6.3039\n",
      "Epoch 33 | Train loss: 6.3032 | Val loss: 6.2963\n",
      "Epoch 34 | Train loss: 6.2893 | Val loss: 6.3043\n",
      "Epoch 35 | Train loss: 6.2785 | Val loss: 6.2864\n",
      "Epoch 36 | Train loss: 6.2702 | Val loss: 6.2661\n",
      "Epoch 37 | Train loss: 6.2719 | Val loss: 6.2649\n",
      "Epoch 38 | Train loss: 6.2790 | Val loss: 6.2689\n",
      "Epoch 39 | Train loss: 6.2715 | Val loss: 6.2606\n",
      "Epoch 40 | Train loss: 6.2690 | Val loss: 6.2572\n",
      "Epoch 41 | Train loss: 6.2488 | Val loss: 6.2528\n",
      "Epoch 42 | Train loss: 6.2266 | Val loss: 6.2638\n",
      "Epoch 43 | Train loss: 6.2393 | Val loss: 6.2505\n",
      "Epoch 44 | Train loss: 6.2553 | Val loss: 6.2506\n",
      "Epoch 45 | Train loss: 6.2412 | Val loss: 6.2396\n",
      "Epoch 46 | Train loss: 6.2436 | Val loss: 6.2308\n",
      "Epoch 47 | Train loss: 6.2301 | Val loss: 6.2308\n",
      "Epoch 48 | Train loss: 6.2152 | Val loss: 6.2418\n",
      "Epoch 49 | Train loss: 6.2073 | Val loss: 6.2376\n",
      "Epoch 50 | Train loss: 6.2078 | Val loss: 6.2274\n",
      "Epoch 51 | Train loss: 6.2163 | Val loss: 6.2426\n",
      "Epoch 52 | Train loss: 6.1913 | Val loss: 6.2172\n",
      "Epoch 53 | Train loss: 6.2095 | Val loss: 6.2151\n",
      "Epoch 54 | Train loss: 6.1873 | Val loss: 6.1977\n",
      "Epoch 55 | Train loss: 6.2009 | Val loss: 6.2088\n",
      "Epoch 56 | Train loss: 6.1929 | Val loss: 6.1978\n",
      "Epoch 57 | Train loss: 6.1884 | Val loss: 6.1849\n",
      "Epoch 58 | Train loss: 6.1739 | Val loss: 6.1780\n",
      "Epoch 59 | Train loss: 6.1845 | Val loss: 6.1670\n",
      "Epoch 60 | Train loss: 6.1708 | Val loss: 6.1913\n",
      "Epoch 61 | Train loss: 6.1810 | Val loss: 6.1704\n",
      "Epoch 62 | Train loss: 6.1459 | Val loss: 6.1379\n",
      "Epoch 63 | Train loss: 6.1472 | Val loss: 6.1291\n",
      "Epoch 64 | Train loss: 6.1437 | Val loss: 6.1625\n",
      "Epoch 65 | Train loss: 6.1332 | Val loss: 6.1478\n",
      "Epoch 66 | Train loss: 6.1035 | Val loss: 6.1241\n",
      "Epoch 67 | Train loss: 6.1242 | Val loss: 6.1198\n",
      "Epoch 68 | Train loss: 6.1192 | Val loss: 6.0962\n",
      "Epoch 69 | Train loss: 6.1099 | Val loss: 6.0845\n",
      "Epoch 70 | Train loss: 6.1038 | Val loss: 6.0958\n",
      "Epoch 71 | Train loss: 6.0904 | Val loss: 6.0967\n",
      "Epoch 72 | Train loss: 6.0550 | Val loss: 6.0768\n",
      "Epoch 73 | Train loss: 6.0623 | Val loss: 6.0703\n",
      "Epoch 74 | Train loss: 6.0707 | Val loss: 6.0672\n",
      "Epoch 75 | Train loss: 6.0359 | Val loss: 6.0223\n",
      "Epoch 76 | Train loss: 6.0285 | Val loss: 6.0306\n",
      "Epoch 77 | Train loss: 6.0346 | Val loss: 6.0167\n",
      "Epoch 78 | Train loss: 5.9968 | Val loss: 6.0055\n",
      "Epoch 79 | Train loss: 6.0089 | Val loss: 5.9899\n",
      "Epoch 80 | Train loss: 5.9848 | Val loss: 5.9767\n",
      "Epoch 81 | Train loss: 5.9857 | Val loss: 5.9562\n",
      "Epoch 82 | Train loss: 5.9689 | Val loss: 5.9543\n",
      "Epoch 83 | Train loss: 5.9415 | Val loss: 5.9345\n",
      "Epoch 84 | Train loss: 5.9305 | Val loss: 5.9382\n",
      "Epoch 85 | Train loss: 5.9349 | Val loss: 5.9214\n",
      "Epoch 86 | Train loss: 5.9094 | Val loss: 5.8945\n",
      "Epoch 87 | Train loss: 5.9098 | Val loss: 5.8968\n",
      "Epoch 88 | Train loss: 5.8821 | Val loss: 5.8850\n",
      "Epoch 89 | Train loss: 5.8739 | Val loss: 5.8530\n",
      "Epoch 90 | Train loss: 5.8665 | Val loss: 5.8625\n",
      "Epoch 91 | Train loss: 5.8459 | Val loss: 5.8443\n",
      "Epoch 92 | Train loss: 5.8225 | Val loss: 5.8233\n",
      "Epoch 93 | Train loss: 5.8097 | Val loss: 5.8245\n",
      "Epoch 94 | Train loss: 5.7873 | Val loss: 5.7738\n",
      "Epoch 95 | Train loss: 5.7731 | Val loss: 5.7627\n",
      "Epoch 96 | Train loss: 5.7688 | Val loss: 5.7499\n",
      "Epoch 97 | Train loss: 5.7643 | Val loss: 5.7607\n",
      "Epoch 98 | Train loss: 5.7385 | Val loss: 5.7157\n",
      "Epoch 99 | Train loss: 5.7509 | Val loss: 5.7082\n",
      "Epoch 100 | Train loss: 5.7231 | Val loss: 5.6985\n",
      "Epoch 101 | Train loss: 5.7241 | Val loss: 5.6652\n",
      "Epoch 102 | Train loss: 5.6875 | Val loss: 5.6786\n",
      "Epoch 103 | Train loss: 5.6859 | Val loss: 5.6631\n",
      "Epoch 104 | Train loss: 5.6529 | Val loss: 5.6417\n",
      "Epoch 105 | Train loss: 5.6483 | Val loss: 5.6455\n",
      "Epoch 106 | Train loss: 5.6058 | Val loss: 5.6578\n",
      "Epoch 107 | Train loss: 5.6216 | Val loss: 5.6423\n",
      "Epoch 108 | Train loss: 5.6314 | Val loss: 5.5988\n",
      "Epoch 109 | Train loss: 5.6091 | Val loss: 5.5870\n",
      "Epoch 110 | Train loss: 5.5987 | Val loss: 5.5762\n",
      "Epoch 111 | Train loss: 5.5745 | Val loss: 5.5776\n",
      "Epoch 112 | Train loss: 5.5555 | Val loss: 5.5443\n",
      "Epoch 113 | Train loss: 5.5611 | Val loss: 5.5190\n",
      "Epoch 114 | Train loss: 5.5368 | Val loss: 5.5304\n",
      "Epoch 115 | Train loss: 5.5383 | Val loss: 5.5007\n",
      "Epoch 116 | Train loss: 5.5311 | Val loss: 5.5035\n",
      "Epoch 117 | Train loss: 5.5137 | Val loss: 5.5108\n",
      "Epoch 118 | Train loss: 5.4990 | Val loss: 5.4858\n",
      "Epoch 119 | Train loss: 5.5025 | Val loss: 5.4674\n",
      "Epoch 120 | Train loss: 5.4733 | Val loss: 5.4629\n",
      "Epoch 121 | Train loss: 5.4786 | Val loss: 5.4321\n",
      "Epoch 122 | Train loss: 5.4669 | Val loss: 5.4201\n",
      "Epoch 123 | Train loss: 5.4605 | Val loss: 5.4632\n",
      "Epoch 124 | Train loss: 5.4434 | Val loss: 5.3944\n",
      "Epoch 125 | Train loss: 5.4268 | Val loss: 5.4282\n",
      "Epoch 126 | Train loss: 5.4149 | Val loss: 5.3930\n",
      "Epoch 127 | Train loss: 5.3963 | Val loss: 5.3934\n",
      "Epoch 128 | Train loss: 5.3809 | Val loss: 5.3478\n",
      "Epoch 129 | Train loss: 5.3860 | Val loss: 5.3581\n",
      "Epoch 130 | Train loss: 5.3611 | Val loss: 5.3585\n",
      "Epoch 131 | Train loss: 5.3268 | Val loss: 5.3319\n",
      "Epoch 132 | Train loss: 5.3467 | Val loss: 5.3301\n",
      "Epoch 133 | Train loss: 5.3391 | Val loss: 5.3370\n",
      "Epoch 134 | Train loss: 5.2907 | Val loss: 5.3190\n",
      "Epoch 135 | Train loss: 5.3449 | Val loss: 5.3197\n",
      "Epoch 136 | Train loss: 5.3026 | Val loss: 5.3025\n",
      "Epoch 137 | Train loss: 5.2929 | Val loss: 5.3267\n",
      "Epoch 138 | Train loss: 5.2789 | Val loss: 5.2513\n",
      "Epoch 139 | Train loss: 5.2454 | Val loss: 5.2877\n",
      "Epoch 140 | Train loss: 5.2692 | Val loss: 5.3063\n",
      "Epoch 141 | Train loss: 5.2196 | Val loss: 5.2534\n",
      "Epoch 142 | Train loss: 5.2532 | Val loss: 5.2457\n",
      "Epoch 143 | Train loss: 5.2346 | Val loss: 5.2482\n",
      "Epoch 144 | Train loss: 5.2365 | Val loss: 5.2699\n",
      "Epoch 145 | Train loss: 5.2182 | Val loss: 5.2158\n",
      "Epoch 146 | Train loss: 5.2393 | Val loss: 5.1889\n",
      "Epoch 147 | Train loss: 5.1777 | Val loss: 5.1943\n",
      "Epoch 148 | Train loss: 5.1845 | Val loss: 5.2078\n",
      "Epoch 149 | Train loss: 5.1732 | Val loss: 5.1901\n",
      "Epoch 150 | Train loss: 5.1293 | Val loss: 5.1943\n",
      "Epoch 151 | Train loss: 5.1747 | Val loss: 5.1793\n",
      "Epoch 152 | Train loss: 5.1550 | Val loss: 5.1564\n",
      "Epoch 153 | Train loss: 5.1460 | Val loss: 5.1581\n",
      "Epoch 154 | Train loss: 5.1623 | Val loss: 5.1576\n",
      "Epoch 155 | Train loss: 5.1081 | Val loss: 5.1354\n",
      "Epoch 156 | Train loss: 5.1023 | Val loss: 5.1322\n",
      "Epoch 157 | Train loss: 5.1071 | Val loss: 5.1365\n",
      "Epoch 158 | Train loss: 5.1175 | Val loss: 5.1124\n",
      "Epoch 159 | Train loss: 5.1276 | Val loss: 5.1370\n",
      "Epoch 160 | Train loss: 5.0973 | Val loss: 5.1246\n",
      "Epoch 161 | Train loss: 5.1094 | Val loss: 5.1165\n",
      "Epoch 162 | Train loss: 5.0885 | Val loss: 5.1090\n",
      "Epoch 163 | Train loss: 5.1003 | Val loss: 5.1249\n",
      "Epoch 164 | Train loss: 5.0530 | Val loss: 5.1013\n",
      "Epoch 165 | Train loss: 5.0403 | Val loss: 5.0999\n",
      "Epoch 166 | Train loss: 5.0738 | Val loss: 5.0703\n",
      "Epoch 167 | Train loss: 5.0272 | Val loss: 5.0481\n",
      "Epoch 168 | Train loss: 5.0292 | Val loss: 5.0708\n",
      "Epoch 169 | Train loss: 5.0090 | Val loss: 5.0531\n",
      "Epoch 170 | Train loss: 5.0233 | Val loss: 5.0679\n",
      "Epoch 171 | Train loss: 5.0331 | Val loss: 5.0903\n",
      "Epoch 172 | Train loss: 4.9931 | Val loss: 5.0609\n",
      "Epoch 173 | Train loss: 4.9875 | Val loss: 5.0777\n",
      "Epoch 174 | Train loss: 4.9978 | Val loss: 5.0060\n",
      "Epoch 175 | Train loss: 5.0042 | Val loss: 5.0322\n",
      "Epoch 176 | Train loss: 4.9600 | Val loss: 5.0238\n",
      "Epoch 177 | Train loss: 4.9782 | Val loss: 5.0319\n",
      "Epoch 178 | Train loss: 4.9103 | Val loss: 4.9990\n",
      "Epoch 179 | Train loss: 4.9280 | Val loss: 5.0158\n",
      "Epoch 180 | Train loss: 4.9963 | Val loss: 5.0129\n",
      "Epoch 181 | Train loss: 4.9798 | Val loss: 4.9902\n",
      "Epoch 182 | Train loss: 4.9595 | Val loss: 5.0251\n",
      "Epoch 183 | Train loss: 4.9170 | Val loss: 4.9613\n",
      "Epoch 184 | Train loss: 4.9498 | Val loss: 4.9972\n",
      "Epoch 185 | Train loss: 4.9246 | Val loss: 4.9988\n",
      "Epoch 186 | Train loss: 4.9469 | Val loss: 4.9738\n",
      "Epoch 187 | Train loss: 4.9141 | Val loss: 4.9644\n",
      "Epoch 188 | Train loss: 4.9332 | Val loss: 4.9608\n",
      "Epoch 189 | Train loss: 4.8712 | Val loss: 4.9842\n",
      "Epoch 190 | Train loss: 4.9119 | Val loss: 4.9206\n",
      "Epoch 191 | Train loss: 4.8735 | Val loss: 4.9632\n",
      "Epoch 192 | Train loss: 4.8903 | Val loss: 4.9489\n",
      "Epoch 193 | Train loss: 4.8851 | Val loss: 4.9568\n",
      "Epoch 194 | Train loss: 4.8571 | Val loss: 4.9462\n",
      "Epoch 195 | Train loss: 4.8674 | Val loss: 4.9413\n",
      "Epoch 196 | Train loss: 4.8625 | Val loss: 4.9299\n",
      "Epoch 197 | Train loss: 4.8662 | Val loss: 4.9256\n",
      "Epoch 198 | Train loss: 4.8686 | Val loss: 4.9399\n",
      "Epoch 199 | Train loss: 4.8230 | Val loss: 4.9442\n",
      "Epoch 200 | Train loss: 4.8621 | Val loss: 4.8766\n",
      "Epoch 201 | Train loss: 4.8501 | Val loss: 4.9180\n",
      "Epoch 202 | Train loss: 4.8105 | Val loss: 4.9230\n",
      "Epoch 203 | Train loss: 4.8544 | Val loss: 4.9172\n",
      "Epoch 204 | Train loss: 4.8593 | Val loss: 4.9087\n",
      "Epoch 205 | Train loss: 4.8122 | Val loss: 4.8821\n",
      "Epoch 206 | Train loss: 4.8491 | Val loss: 4.8900\n",
      "Epoch 207 | Train loss: 4.8059 | Val loss: 4.8981\n",
      "Epoch 208 | Train loss: 4.8211 | Val loss: 4.9211\n",
      "Epoch 209 | Train loss: 4.8783 | Val loss: 4.8745\n",
      "Epoch 210 | Train loss: 4.7790 | Val loss: 4.8830\n",
      "Epoch 211 | Train loss: 4.8232 | Val loss: 4.8958\n",
      "Epoch 212 | Train loss: 4.8097 | Val loss: 4.8584\n",
      "Epoch 213 | Train loss: 4.7840 | Val loss: 4.8745\n",
      "Epoch 214 | Train loss: 4.7713 | Val loss: 4.8915\n",
      "Epoch 215 | Train loss: 4.8108 | Val loss: 4.8698\n",
      "Epoch 216 | Train loss: 4.7813 | Val loss: 4.8968\n",
      "Epoch 217 | Train loss: 4.7824 | Val loss: 4.8862\n",
      "Epoch 218 | Train loss: 4.7769 | Val loss: 4.8958\n",
      "Epoch 219 | Train loss: 4.7949 | Val loss: 4.8772\n",
      "Epoch 220 | Train loss: 4.7513 | Val loss: 4.8714\n",
      "Epoch 221 | Train loss: 4.7858 | Val loss: 4.8822\n",
      "Epoch 222 | Train loss: 4.7243 | Val loss: 4.8631\n",
      "Epoch 223 | Train loss: 4.7493 | Val loss: 4.8392\n",
      "Epoch 224 | Train loss: 4.7694 | Val loss: 4.8629\n",
      "Epoch 225 | Train loss: 4.7191 | Val loss: 4.8625\n",
      "Epoch 226 | Train loss: 4.7409 | Val loss: 4.8399\n",
      "Epoch 227 | Train loss: 4.7595 | Val loss: 4.8842\n",
      "Epoch 228 | Train loss: 4.7286 | Val loss: 4.8548\n",
      "Epoch 229 | Train loss: 4.7158 | Val loss: 4.8445\n",
      "Epoch 230 | Train loss: 4.7275 | Val loss: 4.8595\n",
      "Epoch 231 | Train loss: 4.7405 | Val loss: 4.8777\n",
      "Epoch 232 | Train loss: 4.7255 | Val loss: 4.8497\n",
      "Epoch 233 | Train loss: 4.7068 | Val loss: 4.8491\n",
      "Epoch 234 | Train loss: 4.7321 | Val loss: 4.8491\n",
      "Epoch 235 | Train loss: 4.7398 | Val loss: 4.8267\n",
      "Epoch 236 | Train loss: 4.7103 | Val loss: 4.8272\n",
      "Epoch 237 | Train loss: 4.7315 | Val loss: 4.8412\n",
      "Epoch 238 | Train loss: 4.7285 | Val loss: 4.8373\n",
      "Epoch 239 | Train loss: 4.7056 | Val loss: 4.8366\n",
      "Epoch 240 | Train loss: 4.7116 | Val loss: 4.8431\n",
      "Epoch 241 | Train loss: 4.7059 | Val loss: 4.8304\n",
      "Epoch 242 | Train loss: 4.7079 | Val loss: 4.7989\n",
      "Epoch 243 | Train loss: 4.7220 | Val loss: 4.8327\n",
      "Epoch 244 | Train loss: 4.7076 | Val loss: 4.8179\n",
      "Epoch 245 | Train loss: 4.6897 | Val loss: 4.8329\n",
      "Epoch 246 | Train loss: 4.6827 | Val loss: 4.8542\n",
      "Epoch 247 | Train loss: 4.6907 | Val loss: 4.8005\n",
      "Epoch 248 | Train loss: 4.6607 | Val loss: 4.7795\n",
      "Epoch 249 | Train loss: 4.7112 | Val loss: 4.8419\n",
      "Epoch 250 | Train loss: 4.6434 | Val loss: 4.8267\n",
      "Epoch 251 | Train loss: 4.7001 | Val loss: 4.8151\n",
      "Epoch 252 | Train loss: 4.6518 | Val loss: 4.8047\n",
      "Epoch 253 | Train loss: 4.6554 | Val loss: 4.8020\n",
      "Epoch 254 | Train loss: 4.6517 | Val loss: 4.8175\n",
      "Epoch 255 | Train loss: 4.6826 | Val loss: 4.7803\n",
      "Epoch 256 | Train loss: 4.6612 | Val loss: 4.7859\n",
      "Epoch 257 | Train loss: 4.6486 | Val loss: 4.7761\n",
      "Epoch 258 | Train loss: 4.6550 | Val loss: 4.7525\n",
      "Epoch 259 | Train loss: 4.6662 | Val loss: 4.7679\n",
      "Epoch 260 | Train loss: 4.6085 | Val loss: 4.7693\n",
      "Epoch 261 | Train loss: 4.6354 | Val loss: 4.7407\n",
      "Epoch 262 | Train loss: 4.6627 | Val loss: 4.7692\n",
      "Epoch 263 | Train loss: 4.6476 | Val loss: 4.7942\n",
      "Epoch 264 | Train loss: 4.6539 | Val loss: 4.7597\n",
      "Epoch 265 | Train loss: 4.6228 | Val loss: 4.7380\n",
      "Epoch 266 | Train loss: 4.6421 | Val loss: 4.7632\n",
      "Epoch 267 | Train loss: 4.6240 | Val loss: 4.7290\n",
      "Epoch 268 | Train loss: 4.6283 | Val loss: 4.7174\n",
      "Epoch 269 | Train loss: 4.6443 | Val loss: 4.7488\n",
      "Epoch 270 | Train loss: 4.6038 | Val loss: 4.7688\n",
      "Epoch 271 | Train loss: 4.6159 | Val loss: 4.7338\n",
      "Epoch 272 | Train loss: 4.6616 | Val loss: 4.7366\n",
      "Epoch 273 | Train loss: 4.6055 | Val loss: 4.7266\n",
      "Epoch 274 | Train loss: 4.5960 | Val loss: 4.7654\n",
      "Epoch 275 | Train loss: 4.6239 | Val loss: 4.7386\n",
      "Epoch 276 | Train loss: 4.5794 | Val loss: 4.7696\n",
      "Epoch 277 | Train loss: 4.6366 | Val loss: 4.7524\n",
      "Epoch 278 | Train loss: 4.6178 | Val loss: 4.7229\n",
      "Epoch 279 | Train loss: 4.6335 | Val loss: 4.6876\n",
      "Epoch 280 | Train loss: 4.6022 | Val loss: 4.7550\n",
      "Epoch 281 | Train loss: 4.5993 | Val loss: 4.7545\n",
      "Epoch 282 | Train loss: 4.6034 | Val loss: 4.7655\n",
      "Epoch 283 | Train loss: 4.5949 | Val loss: 4.7489\n",
      "Epoch 284 | Train loss: 4.5848 | Val loss: 4.7582\n",
      "Epoch 285 | Train loss: 4.5626 | Val loss: 4.7042\n",
      "Epoch 286 | Train loss: 4.6077 | Val loss: 4.7310\n",
      "Epoch 287 | Train loss: 4.6172 | Val loss: 4.7397\n",
      "Epoch 288 | Train loss: 4.5787 | Val loss: 4.7154\n",
      "Epoch 289 | Train loss: 4.5856 | Val loss: 4.7790\n",
      "Epoch 290 | Train loss: 4.6157 | Val loss: 4.7540\n",
      "Epoch 291 | Train loss: 4.5958 | Val loss: 4.7217\n",
      "Epoch 292 | Train loss: 4.5880 | Val loss: 4.7130\n",
      "Epoch 293 | Train loss: 4.5880 | Val loss: 4.6597\n",
      "Epoch 294 | Train loss: 4.5699 | Val loss: 4.6765\n",
      "Epoch 295 | Train loss: 4.5693 | Val loss: 4.7262\n",
      "Epoch 296 | Train loss: 4.5925 | Val loss: 4.6748\n",
      "Epoch 297 | Train loss: 4.5585 | Val loss: 4.7175\n",
      "Epoch 298 | Train loss: 4.5551 | Val loss: 4.6918\n",
      "Epoch 299 | Train loss: 4.6111 | Val loss: 4.7017\n",
      "Epoch 300 | Train loss: 4.5692 | Val loss: 4.6599\n",
      "Epoch 301 | Train loss: 4.5322 | Val loss: 4.7096\n",
      "Epoch 302 | Train loss: 4.5453 | Val loss: 4.7048\n",
      "Epoch 303 | Train loss: 4.5820 | Val loss: 4.7028\n",
      "Epoch 304 | Train loss: 4.5661 | Val loss: 4.6718\n",
      "Epoch 305 | Train loss: 4.5929 | Val loss: 4.7265\n",
      "Epoch 306 | Train loss: 4.5692 | Val loss: 4.6697\n",
      "Epoch 307 | Train loss: 4.5341 | Val loss: 4.7243\n",
      "Epoch 308 | Train loss: 4.5570 | Val loss: 4.6800\n",
      "Epoch 309 | Train loss: 4.5991 | Val loss: 4.7132\n",
      "Epoch 310 | Train loss: 4.5589 | Val loss: 4.7156\n",
      "Epoch 311 | Train loss: 4.5403 | Val loss: 4.7000\n",
      "Epoch 312 | Train loss: 4.5420 | Val loss: 4.6933\n",
      "Epoch 313 | Train loss: 4.5280 | Val loss: 4.6892\n",
      "Epoch 314 | Train loss: 4.5168 | Val loss: 4.6659\n",
      "Epoch 315 | Train loss: 4.5398 | Val loss: 4.7105\n",
      "Epoch 316 | Train loss: 4.5186 | Val loss: 4.6774\n",
      "Epoch 317 | Train loss: 4.5209 | Val loss: 4.7307\n",
      "Epoch 318 | Train loss: 4.5341 | Val loss: 4.6578\n",
      "Epoch 319 | Train loss: 4.5798 | Val loss: 4.6447\n",
      "Epoch 320 | Train loss: 4.5479 | Val loss: 4.6943\n",
      "Epoch 321 | Train loss: 4.5433 | Val loss: 4.6844\n",
      "Epoch 322 | Train loss: 4.5109 | Val loss: 4.6821\n",
      "Epoch 323 | Train loss: 4.5163 | Val loss: 4.6564\n",
      "Epoch 324 | Train loss: 4.5112 | Val loss: 4.6759\n",
      "Epoch 325 | Train loss: 4.5139 | Val loss: 4.7058\n",
      "Epoch 326 | Train loss: 4.5354 | Val loss: 4.6836\n",
      "Epoch 327 | Train loss: 4.5361 | Val loss: 4.6685\n",
      "Epoch 328 | Train loss: 4.4653 | Val loss: 4.7013\n",
      "Epoch 329 | Train loss: 4.5568 | Val loss: 4.6917\n",
      "Epoch 330 | Train loss: 4.5090 | Val loss: 4.6841\n",
      "Epoch 331 | Train loss: 4.5268 | Val loss: 4.6990\n",
      "Epoch 332 | Train loss: 4.5051 | Val loss: 4.6819\n",
      "Epoch 333 | Train loss: 4.4957 | Val loss: 4.6892\n",
      "Epoch 334 | Train loss: 4.5215 | Val loss: 4.6659\n",
      "Epoch 335 | Train loss: 4.5373 | Val loss: 4.6544\n",
      "Epoch 336 | Train loss: 4.4684 | Val loss: 4.6672\n",
      "Epoch 337 | Train loss: 4.5426 | Val loss: 4.6941\n",
      "Epoch 338 | Train loss: 4.5046 | Val loss: 4.6561\n",
      "Epoch 339 | Train loss: 4.4896 | Val loss: 4.6522\n",
      "Epoch 340 | Train loss: 4.4847 | Val loss: 4.6568\n",
      "Epoch 341 | Train loss: 4.5234 | Val loss: 4.6511\n",
      "Epoch 342 | Train loss: 4.5208 | Val loss: 4.6840\n",
      "Epoch 343 | Train loss: 4.4906 | Val loss: 4.6581\n",
      "Epoch 344 | Train loss: 4.5106 | Val loss: 4.6592\n",
      "Epoch 345 | Train loss: 4.5098 | Val loss: 4.6386\n",
      "Epoch 346 | Train loss: 4.4662 | Val loss: 4.6080\n",
      "Epoch 347 | Train loss: 4.5150 | Val loss: 4.6533\n",
      "Epoch 348 | Train loss: 4.5210 | Val loss: 4.6763\n",
      "Epoch 349 | Train loss: 4.4708 | Val loss: 4.6545\n",
      "Epoch 350 | Train loss: 4.5131 | Val loss: 4.6194\n",
      "Epoch 351 | Train loss: 4.4529 | Val loss: 4.6526\n",
      "Epoch 352 | Train loss: 4.5031 | Val loss: 4.6088\n",
      "Epoch 353 | Train loss: 4.4592 | Val loss: 4.6099\n",
      "Epoch 354 | Train loss: 4.4830 | Val loss: 4.6478\n",
      "Epoch 355 | Train loss: 4.4703 | Val loss: 4.6187\n",
      "Epoch 356 | Train loss: 4.4607 | Val loss: 4.6342\n",
      "Epoch 357 | Train loss: 4.4708 | Val loss: 4.6379\n",
      "Epoch 358 | Train loss: 4.4915 | Val loss: 4.6081\n",
      "Epoch 359 | Train loss: 4.4840 | Val loss: 4.6372\n",
      "Epoch 360 | Train loss: 4.4842 | Val loss: 4.6360\n",
      "Epoch 361 | Train loss: 4.4535 | Val loss: 4.6357\n",
      "Epoch 362 | Train loss: 4.4605 | Val loss: 4.6238\n",
      "Epoch 363 | Train loss: 4.4728 | Val loss: 4.5950\n",
      "Epoch 364 | Train loss: 4.4529 | Val loss: 4.6395\n",
      "Epoch 365 | Train loss: 4.4766 | Val loss: 4.6461\n",
      "Epoch 366 | Train loss: 4.4702 | Val loss: 4.6765\n",
      "Epoch 367 | Train loss: 4.5023 | Val loss: 4.6338\n",
      "Epoch 368 | Train loss: 4.4402 | Val loss: 4.6416\n",
      "Epoch 369 | Train loss: 4.4829 | Val loss: 4.6445\n",
      "Epoch 370 | Train loss: 4.4820 | Val loss: 4.6132\n",
      "Epoch 371 | Train loss: 4.4494 | Val loss: 4.6276\n",
      "Epoch 372 | Train loss: 4.4192 | Val loss: 4.6634\n",
      "Epoch 373 | Train loss: 4.4649 | Val loss: 4.6599\n",
      "Epoch 374 | Train loss: 4.4652 | Val loss: 4.6330\n",
      "Epoch 375 | Train loss: 4.4250 | Val loss: 4.6470\n",
      "Epoch 376 | Train loss: 4.4205 | Val loss: 4.6058\n",
      "Epoch 377 | Train loss: 4.4337 | Val loss: 4.6307\n",
      "Epoch 378 | Train loss: 4.4462 | Val loss: 4.6525\n",
      "Epoch 379 | Train loss: 4.4411 | Val loss: 4.6362\n",
      "Epoch 380 | Train loss: 4.4943 | Val loss: 4.6065\n",
      "Epoch 381 | Train loss: 4.4370 | Val loss: 4.6033\n",
      "Epoch 382 | Train loss: 4.4486 | Val loss: 4.6510\n",
      "Epoch 383 | Train loss: 4.4456 | Val loss: 4.6392\n",
      "Epoch 384 | Train loss: 4.4728 | Val loss: 4.6057\n",
      "Epoch 385 | Train loss: 4.4321 | Val loss: 4.6209\n",
      "Epoch 386 | Train loss: 4.4434 | Val loss: 4.6061\n",
      "Epoch 387 | Train loss: 4.4443 | Val loss: 4.6251\n",
      "Epoch 388 | Train loss: 4.3783 | Val loss: 4.6325\n",
      "Epoch 389 | Train loss: 4.4138 | Val loss: 4.5888\n",
      "Epoch 390 | Train loss: 4.4567 | Val loss: 4.5285\n",
      "Epoch 391 | Train loss: 4.4649 | Val loss: 4.6021\n",
      "Epoch 392 | Train loss: 4.4325 | Val loss: 4.6375\n",
      "Epoch 393 | Train loss: 4.4505 | Val loss: 4.5879\n",
      "Epoch 394 | Train loss: 4.4446 | Val loss: 4.5591\n",
      "Epoch 395 | Train loss: 4.4622 | Val loss: 4.6308\n",
      "Epoch 396 | Train loss: 4.4257 | Val loss: 4.6603\n",
      "Epoch 397 | Train loss: 4.4194 | Val loss: 4.5703\n",
      "Epoch 398 | Train loss: 4.4326 | Val loss: 4.5987\n",
      "Epoch 399 | Train loss: 4.4206 | Val loss: 4.6238\n",
      "Epoch 400 | Train loss: 4.4189 | Val loss: 4.6086\n",
      "Epoch 401 | Train loss: 4.4456 | Val loss: 4.5754\n",
      "Epoch 402 | Train loss: 4.4397 | Val loss: 4.5585\n",
      "Epoch 403 | Train loss: 4.3743 | Val loss: 4.6002\n",
      "Epoch 404 | Train loss: 4.4577 | Val loss: 4.5865\n",
      "Epoch 405 | Train loss: 4.4363 | Val loss: 4.6146\n",
      "Epoch 406 | Train loss: 4.4146 | Val loss: 4.5953\n",
      "Epoch 407 | Train loss: 4.4396 | Val loss: 4.5433\n",
      "Epoch 408 | Train loss: 4.4126 | Val loss: 4.5958\n",
      "Epoch 409 | Train loss: 4.4200 | Val loss: 4.5795\n",
      "Epoch 410 | Train loss: 4.4497 | Val loss: 4.5973\n",
      "Epoch 411 | Train loss: 4.3930 | Val loss: 4.5729\n",
      "Epoch 412 | Train loss: 4.3976 | Val loss: 4.6425\n",
      "Epoch 413 | Train loss: 4.4066 | Val loss: 4.5706\n",
      "Epoch 414 | Train loss: 4.3770 | Val loss: 4.6481\n",
      "Epoch 415 | Train loss: 4.4367 | Val loss: 4.5572\n",
      "Epoch 416 | Train loss: 4.3981 | Val loss: 4.5942\n",
      "Epoch 417 | Train loss: 4.3982 | Val loss: 4.6143\n",
      "Epoch 418 | Train loss: 4.3796 | Val loss: 4.6280\n",
      "Epoch 419 | Train loss: 4.4509 | Val loss: 4.6163\n",
      "Epoch 420 | Train loss: 4.3971 | Val loss: 4.5502\n",
      "Epoch 421 | Train loss: 4.4290 | Val loss: 4.6220\n",
      "Epoch 422 | Train loss: 4.4089 | Val loss: 4.5578\n",
      "Epoch 423 | Train loss: 4.3732 | Val loss: 4.6148\n",
      "Epoch 424 | Train loss: 4.3702 | Val loss: 4.5625\n",
      "Epoch 425 | Train loss: 4.4466 | Val loss: 4.5675\n",
      "Epoch 426 | Train loss: 4.3552 | Val loss: 4.5677\n",
      "Epoch 427 | Train loss: 4.3516 | Val loss: 4.6034\n",
      "Epoch 428 | Train loss: 4.3991 | Val loss: 4.5691\n",
      "Epoch 429 | Train loss: 4.3784 | Val loss: 4.5765\n",
      "Epoch 430 | Train loss: 4.3798 | Val loss: 4.6072\n",
      "Epoch 431 | Train loss: 4.3959 | Val loss: 4.5579\n",
      "Epoch 432 | Train loss: 4.4014 | Val loss: 4.6046\n",
      "Epoch 433 | Train loss: 4.3914 | Val loss: 4.5384\n",
      "Epoch 434 | Train loss: 4.3775 | Val loss: 4.5637\n",
      "Epoch 435 | Train loss: 4.4285 | Val loss: 4.5262\n",
      "Epoch 436 | Train loss: 4.3632 | Val loss: 4.6006\n",
      "Epoch 437 | Train loss: 4.3898 | Val loss: 4.5717\n",
      "Epoch 438 | Train loss: 4.4099 | Val loss: 4.5545\n",
      "Epoch 439 | Train loss: 4.3697 | Val loss: 4.5837\n",
      "Epoch 440 | Train loss: 4.3806 | Val loss: 4.5778\n",
      "Epoch 441 | Train loss: 4.3960 | Val loss: 4.5686\n",
      "Epoch 442 | Train loss: 4.3625 | Val loss: 4.5590\n",
      "Epoch 443 | Train loss: 4.3717 | Val loss: 4.5848\n",
      "Epoch 444 | Train loss: 4.4116 | Val loss: 4.5889\n",
      "Epoch 445 | Train loss: 4.3742 | Val loss: 4.5781\n",
      "Epoch 446 | Train loss: 4.3955 | Val loss: 4.5822\n",
      "Epoch 447 | Train loss: 4.3661 | Val loss: 4.5998\n",
      "Epoch 448 | Train loss: 4.3872 | Val loss: 4.5808\n",
      "Epoch 449 | Train loss: 4.3516 | Val loss: 4.5797\n",
      "Epoch 450 | Train loss: 4.3695 | Val loss: 4.5574\n",
      "Epoch 451 | Train loss: 4.3835 | Val loss: 4.6124\n",
      "Epoch 452 | Train loss: 4.4102 | Val loss: 4.5766\n",
      "Epoch 453 | Train loss: 4.3953 | Val loss: 4.5971\n",
      "Epoch 454 | Train loss: 4.3296 | Val loss: 4.5674\n",
      "Epoch 455 | Train loss: 4.3764 | Val loss: 4.5844\n",
      "Epoch 456 | Train loss: 4.3456 | Val loss: 4.5404\n",
      "Epoch 457 | Train loss: 4.3546 | Val loss: 4.5892\n",
      "Epoch 458 | Train loss: 4.3692 | Val loss: 4.5205\n",
      "Epoch 459 | Train loss: 4.3551 | Val loss: 4.5206\n",
      "Epoch 460 | Train loss: 4.3642 | Val loss: 4.5595\n",
      "Epoch 461 | Train loss: 4.3728 | Val loss: 4.5637\n",
      "Epoch 462 | Train loss: 4.3940 | Val loss: 4.5501\n",
      "Epoch 463 | Train loss: 4.3752 | Val loss: 4.5891\n",
      "Epoch 464 | Train loss: 4.3693 | Val loss: 4.5251\n",
      "Epoch 465 | Train loss: 4.3237 | Val loss: 4.5264\n",
      "Epoch 466 | Train loss: 4.3552 | Val loss: 4.5704\n",
      "Epoch 467 | Train loss: 4.3680 | Val loss: 4.5299\n",
      "Epoch 468 | Train loss: 4.3749 | Val loss: 4.5511\n",
      "Epoch 469 | Train loss: 4.3642 | Val loss: 4.5554\n",
      "Epoch 470 | Train loss: 4.3313 | Val loss: 4.5698\n",
      "Epoch 471 | Train loss: 4.3598 | Val loss: 4.5755\n",
      "Epoch 472 | Train loss: 4.3701 | Val loss: 4.5343\n",
      "Epoch 473 | Train loss: 4.3904 | Val loss: 4.5782\n",
      "Epoch 474 | Train loss: 4.3796 | Val loss: 4.5760\n",
      "Epoch 475 | Train loss: 4.3425 | Val loss: 4.5463\n",
      "Epoch 476 | Train loss: 4.3548 | Val loss: 4.5308\n",
      "Epoch 477 | Train loss: 4.3634 | Val loss: 4.5433\n",
      "Epoch 478 | Train loss: 4.3705 | Val loss: 4.5228\n",
      "Epoch 479 | Train loss: 4.3706 | Val loss: 4.4967\n",
      "Epoch 480 | Train loss: 4.3528 | Val loss: 4.5059\n",
      "Epoch 481 | Train loss: 4.3110 | Val loss: 4.5601\n",
      "Epoch 482 | Train loss: 4.3411 | Val loss: 4.5411\n",
      "Epoch 483 | Train loss: 4.3612 | Val loss: 4.5946\n",
      "Epoch 484 | Train loss: 4.3789 | Val loss: 4.5588\n",
      "Epoch 485 | Train loss: 4.3436 | Val loss: 4.5497\n",
      "Epoch 486 | Train loss: 4.2991 | Val loss: 4.5389\n",
      "Epoch 487 | Train loss: 4.3541 | Val loss: 4.5243\n",
      "Epoch 488 | Train loss: 4.3550 | Val loss: 4.5259\n",
      "Epoch 489 | Train loss: 4.3030 | Val loss: 4.5519\n",
      "Epoch 490 | Train loss: 4.3402 | Val loss: 4.5512\n",
      "Epoch 491 | Train loss: 4.3624 | Val loss: 4.5798\n",
      "Epoch 492 | Train loss: 4.3130 | Val loss: 4.5535\n",
      "Epoch 493 | Train loss: 4.3374 | Val loss: 4.5764\n",
      "Epoch 494 | Train loss: 4.3192 | Val loss: 4.5408\n",
      "Epoch 495 | Train loss: 4.3067 | Val loss: 4.5474\n",
      "Epoch 496 | Train loss: 4.3215 | Val loss: 4.5645\n",
      "Epoch 497 | Train loss: 4.3542 | Val loss: 4.5485\n",
      "Epoch 498 | Train loss: 4.3443 | Val loss: 4.4920\n",
      "Epoch 499 | Train loss: 4.3359 | Val loss: 4.5497\n",
      "Epoch 500 | Train loss: 4.3460 | Val loss: 4.5401\n",
      "Epoch 501 | Train loss: 4.3507 | Val loss: 4.5728\n",
      "Epoch 502 | Train loss: 4.3462 | Val loss: 4.5394\n",
      "Epoch 503 | Train loss: 4.3575 | Val loss: 4.5186\n",
      "Epoch 504 | Train loss: 4.3755 | Val loss: 4.5409\n",
      "Epoch 505 | Train loss: 4.3114 | Val loss: 4.5636\n",
      "Epoch 506 | Train loss: 4.3161 | Val loss: 4.5318\n",
      "Epoch 507 | Train loss: 4.3493 | Val loss: 4.5020\n",
      "Epoch 508 | Train loss: 4.3072 | Val loss: 4.4995\n",
      "Epoch 509 | Train loss: 4.3272 | Val loss: 4.5144\n",
      "Epoch 510 | Train loss: 4.3390 | Val loss: 4.5131\n",
      "Epoch 511 | Train loss: 4.3165 | Val loss: 4.5543\n",
      "Epoch 512 | Train loss: 4.3515 | Val loss: 4.5228\n",
      "Epoch 513 | Train loss: 4.3015 | Val loss: 4.5084\n",
      "Epoch 514 | Train loss: 4.3117 | Val loss: 4.5298\n",
      "Epoch 515 | Train loss: 4.3236 | Val loss: 4.5063\n",
      "Epoch 516 | Train loss: 4.3328 | Val loss: 4.4950\n",
      "Epoch 517 | Train loss: 4.2981 | Val loss: 4.5148\n",
      "Epoch 518 | Train loss: 4.3385 | Val loss: 4.5385\n",
      "Epoch 519 | Train loss: 4.3064 | Val loss: 4.5635\n",
      "Epoch 520 | Train loss: 4.3005 | Val loss: 4.5381\n",
      "Epoch 521 | Train loss: 4.3162 | Val loss: 4.5133\n",
      "Epoch 522 | Train loss: 4.3441 | Val loss: 4.4961\n",
      "Epoch 523 | Train loss: 4.2900 | Val loss: 4.5595\n",
      "Epoch 524 | Train loss: 4.3060 | Val loss: 4.5117\n",
      "Epoch 525 | Train loss: 4.3065 | Val loss: 4.5209\n",
      "Epoch 526 | Train loss: 4.2926 | Val loss: 4.5241\n",
      "Epoch 527 | Train loss: 4.3066 | Val loss: 4.4979\n",
      "Epoch 528 | Train loss: 4.2671 | Val loss: 4.5357\n",
      "Epoch 529 | Train loss: 4.3208 | Val loss: 4.5459\n",
      "Epoch 530 | Train loss: 4.3052 | Val loss: 4.4983\n",
      "Epoch 531 | Train loss: 4.3282 | Val loss: 4.5570\n",
      "Epoch 532 | Train loss: 4.3147 | Val loss: 4.5155\n",
      "Epoch 533 | Train loss: 4.3060 | Val loss: 4.4489\n",
      "Epoch 534 | Train loss: 4.3033 | Val loss: 4.5246\n",
      "Epoch 535 | Train loss: 4.2374 | Val loss: 4.5293\n",
      "Epoch 536 | Train loss: 4.2935 | Val loss: 4.5501\n",
      "Epoch 537 | Train loss: 4.2765 | Val loss: 4.5085\n",
      "Epoch 538 | Train loss: 4.2715 | Val loss: 4.5062\n",
      "Epoch 539 | Train loss: 4.3103 | Val loss: 4.5252\n",
      "Epoch 540 | Train loss: 4.3082 | Val loss: 4.5445\n",
      "Epoch 541 | Train loss: 4.2998 | Val loss: 4.5146\n",
      "Epoch 542 | Train loss: 4.2822 | Val loss: 4.5289\n",
      "Epoch 543 | Train loss: 4.2672 | Val loss: 4.5215\n",
      "Epoch 544 | Train loss: 4.3069 | Val loss: 4.5414\n",
      "Epoch 545 | Train loss: 4.3317 | Val loss: 4.5049\n",
      "Epoch 546 | Train loss: 4.2452 | Val loss: 4.5133\n",
      "Epoch 547 | Train loss: 4.2961 | Val loss: 4.4954\n",
      "Epoch 548 | Train loss: 4.2887 | Val loss: 4.4888\n",
      "Epoch 549 | Train loss: 4.3219 | Val loss: 4.4911\n",
      "Epoch 550 | Train loss: 4.2561 | Val loss: 4.5480\n",
      "Epoch 551 | Train loss: 4.2996 | Val loss: 4.5414\n",
      "Epoch 552 | Train loss: 4.2682 | Val loss: 4.4952\n",
      "Epoch 553 | Train loss: 4.2635 | Val loss: 4.4965\n",
      "Epoch 554 | Train loss: 4.2899 | Val loss: 4.5143\n",
      "Epoch 555 | Train loss: 4.3302 | Val loss: 4.5131\n",
      "Epoch 556 | Train loss: 4.3077 | Val loss: 4.5269\n",
      "Epoch 557 | Train loss: 4.2650 | Val loss: 4.4755\n",
      "Epoch 558 | Train loss: 4.2881 | Val loss: 4.4541\n",
      "Epoch 559 | Train loss: 4.2966 | Val loss: 4.4962\n",
      "Epoch 560 | Train loss: 4.2894 | Val loss: 4.5068\n",
      "Epoch 561 | Train loss: 4.2437 | Val loss: 4.5657\n",
      "Epoch 562 | Train loss: 4.2717 | Val loss: 4.5183\n",
      "Epoch 563 | Train loss: 4.2562 | Val loss: 4.5398\n",
      "Epoch 564 | Train loss: 4.2569 | Val loss: 4.4788\n",
      "Epoch 565 | Train loss: 4.2501 | Val loss: 4.5514\n",
      "Epoch 566 | Train loss: 4.2870 | Val loss: 4.5193\n",
      "Epoch 567 | Train loss: 4.2880 | Val loss: 4.5053\n",
      "Epoch 568 | Train loss: 4.2723 | Val loss: 4.5046\n",
      "Epoch 569 | Train loss: 4.2620 | Val loss: 4.4668\n",
      "Epoch 570 | Train loss: 4.2826 | Val loss: 4.5099\n",
      "Epoch 571 | Train loss: 4.2482 | Val loss: 4.5552\n",
      "Epoch 572 | Train loss: 4.2881 | Val loss: 4.4900\n",
      "Epoch 573 | Train loss: 4.3062 | Val loss: 4.5199\n",
      "Epoch 574 | Train loss: 4.2938 | Val loss: 4.5036\n",
      "Epoch 575 | Train loss: 4.2876 | Val loss: 4.4905\n",
      "Epoch 576 | Train loss: 4.2708 | Val loss: 4.4849\n",
      "Epoch 577 | Train loss: 4.2658 | Val loss: 4.4418\n",
      "Epoch 578 | Train loss: 4.2558 | Val loss: 4.5102\n",
      "Epoch 579 | Train loss: 4.2587 | Val loss: 4.5237\n",
      "Epoch 580 | Train loss: 4.2558 | Val loss: 4.4742\n",
      "Epoch 581 | Train loss: 4.2985 | Val loss: 4.5224\n",
      "Epoch 582 | Train loss: 4.2703 | Val loss: 4.4789\n",
      "Epoch 583 | Train loss: 4.2441 | Val loss: 4.4930\n",
      "Epoch 584 | Train loss: 4.2817 | Val loss: 4.5335\n",
      "Epoch 585 | Train loss: 4.3101 | Val loss: 4.5268\n",
      "Epoch 586 | Train loss: 4.2792 | Val loss: 4.5017\n",
      "Epoch 587 | Train loss: 4.2476 | Val loss: 4.4884\n",
      "Epoch 588 | Train loss: 4.2666 | Val loss: 4.5283\n",
      "Epoch 589 | Train loss: 4.2621 | Val loss: 4.5302\n",
      "Epoch 590 | Train loss: 4.2898 | Val loss: 4.4954\n",
      "Epoch 591 | Train loss: 4.2576 | Val loss: 4.5160\n",
      "Epoch 592 | Train loss: 4.2943 | Val loss: 4.5120\n",
      "Epoch 593 | Train loss: 4.2632 | Val loss: 4.5228\n",
      "Epoch 594 | Train loss: 4.2787 | Val loss: 4.5128\n",
      "Epoch 595 | Train loss: 4.2685 | Val loss: 4.4770\n",
      "Epoch 596 | Train loss: 4.2607 | Val loss: 4.5160\n",
      "Epoch 597 | Train loss: 4.2761 | Val loss: 4.4818\n",
      "Epoch 598 | Train loss: 4.2917 | Val loss: 4.5138\n",
      "Epoch 599 | Train loss: 4.2505 | Val loss: 4.4784\n",
      "Epoch 600 | Train loss: 4.2679 | Val loss: 4.4980\n",
      "Epoch 601 | Train loss: 4.2569 | Val loss: 4.4791\n",
      "Epoch 602 | Train loss: 4.2070 | Val loss: 4.4991\n",
      "Epoch 603 | Train loss: 4.2480 | Val loss: 4.4672\n",
      "Epoch 604 | Train loss: 4.2780 | Val loss: 4.4909\n",
      "Epoch 605 | Train loss: 4.2561 | Val loss: 4.4858\n",
      "Epoch 606 | Train loss: 4.2703 | Val loss: 4.5065\n",
      "Epoch 607 | Train loss: 4.2863 | Val loss: 4.4910\n",
      "Epoch 608 | Train loss: 4.2581 | Val loss: 4.5035\n",
      "Epoch 609 | Train loss: 4.2554 | Val loss: 4.4819\n",
      "Epoch 610 | Train loss: 4.2418 | Val loss: 4.4954\n",
      "Epoch 611 | Train loss: 4.2688 | Val loss: 4.4730\n",
      "Epoch 612 | Train loss: 4.2686 | Val loss: 4.4743\n",
      "Epoch 613 | Train loss: 4.2377 | Val loss: 4.4763\n",
      "Epoch 614 | Train loss: 4.2416 | Val loss: 4.4733\n",
      "Epoch 615 | Train loss: 4.2651 | Val loss: 4.5165\n",
      "Epoch 616 | Train loss: 4.2682 | Val loss: 4.4707\n",
      "Epoch 617 | Train loss: 4.2752 | Val loss: 4.4777\n",
      "Epoch 618 | Train loss: 4.2549 | Val loss: 4.4744\n",
      "Epoch 619 | Train loss: 4.2275 | Val loss: 4.5119\n",
      "Epoch 620 | Train loss: 4.2805 | Val loss: 4.4419\n",
      "Epoch 621 | Train loss: 4.2589 | Val loss: 4.5246\n",
      "Epoch 622 | Train loss: 4.2115 | Val loss: 4.4817\n",
      "Epoch 623 | Train loss: 4.2566 | Val loss: 4.5151\n",
      "Epoch 624 | Train loss: 4.2569 | Val loss: 4.4937\n",
      "Epoch 625 | Train loss: 4.2100 | Val loss: 4.4647\n",
      "Epoch 626 | Train loss: 4.1901 | Val loss: 4.4291\n",
      "Epoch 627 | Train loss: 4.2662 | Val loss: 4.4832\n",
      "Epoch 628 | Train loss: 4.2531 | Val loss: 4.4733\n",
      "Epoch 629 | Train loss: 4.2311 | Val loss: 4.4882\n",
      "Epoch 630 | Train loss: 4.1885 | Val loss: 4.4683\n",
      "Epoch 631 | Train loss: 4.2267 | Val loss: 4.4744\n",
      "Epoch 632 | Train loss: 4.2583 | Val loss: 4.4641\n",
      "Epoch 633 | Train loss: 4.2398 | Val loss: 4.4921\n",
      "Epoch 634 | Train loss: 4.2658 | Val loss: 4.4862\n",
      "Epoch 635 | Train loss: 4.2626 | Val loss: 4.4851\n",
      "Epoch 636 | Train loss: 4.1988 | Val loss: 4.4706\n",
      "Epoch 637 | Train loss: 4.2173 | Val loss: 4.4614\n",
      "Epoch 638 | Train loss: 4.2417 | Val loss: 4.4183\n",
      "Epoch 639 | Train loss: 4.2616 | Val loss: 4.5224\n",
      "Epoch 640 | Train loss: 4.2478 | Val loss: 4.4652\n",
      "Epoch 641 | Train loss: 4.2634 | Val loss: 4.5001\n",
      "Epoch 642 | Train loss: 4.2230 | Val loss: 4.4614\n",
      "Epoch 643 | Train loss: 4.1868 | Val loss: 4.4927\n",
      "Epoch 644 | Train loss: 4.2478 | Val loss: 4.4362\n",
      "Epoch 645 | Train loss: 4.1985 | Val loss: 4.4953\n",
      "Epoch 646 | Train loss: 4.2449 | Val loss: 4.4926\n",
      "Epoch 647 | Train loss: 4.2360 | Val loss: 4.4420\n",
      "Epoch 648 | Train loss: 4.1930 | Val loss: 4.5062\n",
      "Epoch 649 | Train loss: 4.1898 | Val loss: 4.4967\n",
      "Epoch 650 | Train loss: 4.2537 | Val loss: 4.4311\n",
      "Epoch 651 | Train loss: 4.2419 | Val loss: 4.4505\n",
      "Epoch 652 | Train loss: 4.2173 | Val loss: 4.4471\n",
      "Epoch 653 | Train loss: 4.2458 | Val loss: 4.4910\n",
      "Epoch 654 | Train loss: 4.1923 | Val loss: 4.4630\n",
      "Epoch 655 | Train loss: 4.2134 | Val loss: 4.4397\n",
      "Epoch 656 | Train loss: 4.2336 | Val loss: 4.4819\n",
      "Epoch 657 | Train loss: 4.1550 | Val loss: 4.4735\n",
      "Epoch 658 | Train loss: 4.2420 | Val loss: 4.4203\n",
      "Epoch 659 | Train loss: 4.2144 | Val loss: 4.4636\n",
      "Epoch 660 | Train loss: 4.2125 | Val loss: 4.5047\n",
      "Epoch 661 | Train loss: 4.2130 | Val loss: 4.4701\n",
      "Epoch 662 | Train loss: 4.2321 | Val loss: 4.5136\n",
      "Epoch 663 | Train loss: 4.2219 | Val loss: 4.4317\n",
      "Epoch 664 | Train loss: 4.2430 | Val loss: 4.4543\n",
      "Epoch 665 | Train loss: 4.2292 | Val loss: 4.4680\n",
      "Epoch 666 | Train loss: 4.1981 | Val loss: 4.4688\n",
      "Epoch 667 | Train loss: 4.1906 | Val loss: 4.4943\n",
      "Epoch 668 | Train loss: 4.2113 | Val loss: 4.4094\n",
      "Epoch 669 | Train loss: 4.2090 | Val loss: 4.4683\n",
      "Epoch 670 | Train loss: 4.2142 | Val loss: 4.4454\n",
      "Epoch 671 | Train loss: 4.1742 | Val loss: 4.4657\n",
      "Epoch 672 | Train loss: 4.2154 | Val loss: 4.4707\n",
      "Epoch 673 | Train loss: 4.2041 | Val loss: 4.4647\n",
      "Epoch 674 | Train loss: 4.2119 | Val loss: 4.4690\n",
      "Epoch 675 | Train loss: 4.2347 | Val loss: 4.4717\n",
      "Epoch 676 | Train loss: 4.2133 | Val loss: 4.4584\n",
      "Epoch 677 | Train loss: 4.1584 | Val loss: 4.4160\n",
      "Epoch 678 | Train loss: 4.1896 | Val loss: 4.4487\n",
      "Epoch 679 | Train loss: 4.2304 | Val loss: 4.4443\n",
      "Epoch 680 | Train loss: 4.1892 | Val loss: 4.4460\n",
      "Epoch 681 | Train loss: 4.2122 | Val loss: 4.4560\n",
      "Epoch 682 | Train loss: 4.1887 | Val loss: 4.4263\n",
      "Epoch 683 | Train loss: 4.1849 | Val loss: 4.4324\n",
      "Epoch 684 | Train loss: 4.2202 | Val loss: 4.3899\n",
      "Epoch 685 | Train loss: 4.1990 | Val loss: 4.4205\n",
      "Epoch 686 | Train loss: 4.2174 | Val loss: 4.4552\n",
      "Epoch 687 | Train loss: 4.1958 | Val loss: 4.4130\n",
      "Epoch 688 | Train loss: 4.1741 | Val loss: 4.4516\n",
      "Epoch 689 | Train loss: 4.1990 | Val loss: 4.4607\n",
      "Epoch 690 | Train loss: 4.1692 | Val loss: 4.4469\n",
      "Epoch 691 | Train loss: 4.2393 | Val loss: 4.4740\n",
      "Epoch 692 | Train loss: 4.2263 | Val loss: 4.4489\n",
      "Epoch 693 | Train loss: 4.1985 | Val loss: 4.4379\n",
      "Epoch 694 | Train loss: 4.1728 | Val loss: 4.4302\n",
      "Epoch 695 | Train loss: 4.2010 | Val loss: 4.4285\n",
      "Epoch 696 | Train loss: 4.1931 | Val loss: 4.4360\n",
      "Epoch 697 | Train loss: 4.1916 | Val loss: 4.4851\n",
      "Epoch 698 | Train loss: 4.1997 | Val loss: 4.4241\n",
      "Epoch 699 | Train loss: 4.1756 | Val loss: 4.4263\n",
      "Epoch 700 | Train loss: 4.1790 | Val loss: 4.4399\n",
      "Epoch 701 | Train loss: 4.1761 | Val loss: 4.4757\n",
      "Epoch 702 | Train loss: 4.2029 | Val loss: 4.4857\n",
      "Epoch 703 | Train loss: 4.1777 | Val loss: 4.4657\n",
      "Epoch 704 | Train loss: 4.1829 | Val loss: 4.4606\n",
      "Epoch 705 | Train loss: 4.1608 | Val loss: 4.4313\n",
      "Epoch 706 | Train loss: 4.1829 | Val loss: 4.4885\n",
      "Epoch 707 | Train loss: 4.2116 | Val loss: 4.4121\n",
      "Epoch 708 | Train loss: 4.1739 | Val loss: 4.4700\n",
      "Epoch 709 | Train loss: 4.2353 | Val loss: 4.4579\n",
      "Epoch 710 | Train loss: 4.1880 | Val loss: 4.4576\n",
      "Epoch 711 | Train loss: 4.1484 | Val loss: 4.4566\n",
      "Epoch 712 | Train loss: 4.1615 | Val loss: 4.3997\n",
      "Epoch 713 | Train loss: 4.1446 | Val loss: 4.4402\n",
      "Epoch 714 | Train loss: 4.2094 | Val loss: 4.4747\n",
      "Epoch 715 | Train loss: 4.1110 | Val loss: 4.4670\n",
      "Epoch 716 | Train loss: 4.1731 | Val loss: 4.4988\n",
      "Epoch 717 | Train loss: 4.1702 | Val loss: 4.5111\n",
      "Epoch 718 | Train loss: 4.1927 | Val loss: 4.4613\n",
      "Epoch 719 | Train loss: 4.1928 | Val loss: 4.4405\n",
      "Epoch 720 | Train loss: 4.1992 | Val loss: 4.4082\n",
      "Epoch 721 | Train loss: 4.2019 | Val loss: 4.4009\n",
      "Epoch 722 | Train loss: 4.2024 | Val loss: 4.4789\n",
      "Epoch 723 | Train loss: 4.2065 | Val loss: 4.4627\n",
      "Epoch 724 | Train loss: 4.1816 | Val loss: 4.4231\n",
      "Epoch 725 | Train loss: 4.2110 | Val loss: 4.3861\n",
      "Epoch 726 | Train loss: 4.1999 | Val loss: 4.4647\n",
      "Epoch 727 | Train loss: 4.2218 | Val loss: 4.4517\n",
      "Epoch 728 | Train loss: 4.1765 | Val loss: 4.4262\n",
      "Epoch 729 | Train loss: 4.1438 | Val loss: 4.4300\n",
      "Epoch 730 | Train loss: 4.1942 | Val loss: 4.4706\n",
      "Epoch 731 | Train loss: 4.1635 | Val loss: 4.4607\n",
      "Epoch 732 | Train loss: 4.1469 | Val loss: 4.4094\n",
      "Epoch 733 | Train loss: 4.1925 | Val loss: 4.4620\n",
      "Epoch 734 | Train loss: 4.1718 | Val loss: 4.4007\n",
      "Epoch 735 | Train loss: 4.2100 | Val loss: 4.4412\n",
      "Epoch 736 | Train loss: 4.2201 | Val loss: 4.4237\n",
      "Epoch 737 | Train loss: 4.1939 | Val loss: 4.4659\n",
      "Epoch 738 | Train loss: 4.1887 | Val loss: 4.4129\n",
      "Epoch 739 | Train loss: 4.1751 | Val loss: 4.4381\n",
      "Epoch 740 | Train loss: 4.1724 | Val loss: 4.4233\n",
      "Epoch 741 | Train loss: 4.1666 | Val loss: 4.4640\n",
      "Epoch 742 | Train loss: 4.1260 | Val loss: 4.4260\n",
      "Epoch 743 | Train loss: 4.1873 | Val loss: 4.4715\n",
      "Epoch 744 | Train loss: 4.1515 | Val loss: 4.4545\n",
      "Epoch 745 | Train loss: 4.1126 | Val loss: 4.4583\n",
      "Epoch 746 | Train loss: 4.1625 | Val loss: 4.4539\n",
      "Epoch 747 | Train loss: 4.1632 | Val loss: 4.4210\n",
      "Epoch 748 | Train loss: 4.1801 | Val loss: 4.4167\n",
      "Epoch 749 | Train loss: 4.1816 | Val loss: 4.3950\n",
      "Epoch 750 | Train loss: 4.1832 | Val loss: 4.4150\n",
      "Epoch 751 | Train loss: 4.1676 | Val loss: 4.4200\n",
      "Epoch 752 | Train loss: 4.1659 | Val loss: 4.4199\n",
      "Epoch 753 | Train loss: 4.1805 | Val loss: 4.4305\n",
      "Epoch 754 | Train loss: 4.1516 | Val loss: 4.4038\n",
      "Epoch 755 | Train loss: 4.1509 | Val loss: 4.4277\n",
      "Epoch 756 | Train loss: 4.1544 | Val loss: 4.4094\n",
      "Epoch 757 | Train loss: 4.1617 | Val loss: 4.3764\n",
      "Epoch 758 | Train loss: 4.1235 | Val loss: 4.4513\n",
      "Epoch 759 | Train loss: 4.1819 | Val loss: 4.4349\n",
      "Epoch 760 | Train loss: 4.1849 | Val loss: 4.3769\n",
      "Epoch 761 | Train loss: 4.1409 | Val loss: 4.4180\n",
      "Epoch 762 | Train loss: 4.1414 | Val loss: 4.4779\n",
      "Epoch 763 | Train loss: 4.1558 | Val loss: 4.4155\n",
      "Epoch 764 | Train loss: 4.1554 | Val loss: 4.4304\n",
      "Epoch 765 | Train loss: 4.1328 | Val loss: 4.4377\n",
      "Epoch 766 | Train loss: 4.1770 | Val loss: 4.4331\n",
      "Epoch 767 | Train loss: 4.1664 | Val loss: 4.4419\n",
      "Epoch 768 | Train loss: 4.1641 | Val loss: 4.4034\n",
      "Epoch 769 | Train loss: 4.1777 | Val loss: 4.4502\n",
      "Epoch 770 | Train loss: 4.1367 | Val loss: 4.4462\n",
      "Epoch 771 | Train loss: 4.1602 | Val loss: 4.4054\n",
      "Epoch 772 | Train loss: 4.1164 | Val loss: 4.4414\n",
      "Epoch 773 | Train loss: 4.1515 | Val loss: 4.4568\n",
      "Epoch 774 | Train loss: 4.1899 | Val loss: 4.4239\n",
      "Epoch 775 | Train loss: 4.1921 | Val loss: 4.4093\n",
      "Epoch 776 | Train loss: 4.1516 | Val loss: 4.4360\n",
      "Epoch 777 | Train loss: 4.1620 | Val loss: 4.4630\n",
      "Epoch 778 | Train loss: 4.1472 | Val loss: 4.4036\n",
      "Epoch 779 | Train loss: 4.1718 | Val loss: 4.4439\n",
      "Epoch 780 | Train loss: 4.1337 | Val loss: 4.4277\n",
      "Epoch 781 | Train loss: 4.1435 | Val loss: 4.4309\n",
      "Epoch 782 | Train loss: 4.1379 | Val loss: 4.4782\n",
      "Epoch 783 | Train loss: 4.1547 | Val loss: 4.4696\n",
      "Epoch 784 | Train loss: 4.1730 | Val loss: 4.4056\n",
      "Epoch 785 | Train loss: 4.1533 | Val loss: 4.3768\n",
      "Epoch 786 | Train loss: 4.1480 | Val loss: 4.4012\n",
      "Epoch 787 | Train loss: 4.1599 | Val loss: 4.4512\n",
      "Epoch 788 | Train loss: 4.0865 | Val loss: 4.3728\n",
      "Epoch 789 | Train loss: 4.1672 | Val loss: 4.4452\n",
      "Epoch 790 | Train loss: 4.1568 | Val loss: 4.4410\n",
      "Epoch 791 | Train loss: 4.1313 | Val loss: 4.4682\n",
      "Epoch 792 | Train loss: 4.1163 | Val loss: 4.4391\n",
      "Epoch 793 | Train loss: 4.1424 | Val loss: 4.4071\n",
      "Epoch 794 | Train loss: 4.1236 | Val loss: 4.4338\n",
      "Epoch 795 | Train loss: 4.1322 | Val loss: 4.4337\n",
      "Epoch 796 | Train loss: 4.1212 | Val loss: 4.4342\n",
      "Epoch 797 | Train loss: 4.1244 | Val loss: 4.4138\n",
      "Epoch 798 | Train loss: 4.1091 | Val loss: 4.3770\n",
      "Epoch 799 | Train loss: 4.1395 | Val loss: 4.4636\n",
      "Epoch 800 | Train loss: 4.1243 | Val loss: 4.3960\n",
      "Epoch 801 | Train loss: 4.1624 | Val loss: 4.4182\n",
      "Epoch 802 | Train loss: 4.1508 | Val loss: 4.4376\n",
      "Epoch 803 | Train loss: 4.0971 | Val loss: 4.4053\n",
      "Epoch 804 | Train loss: 4.1110 | Val loss: 4.4304\n",
      "Epoch 805 | Train loss: 4.1487 | Val loss: 4.4265\n",
      "Epoch 806 | Train loss: 4.1399 | Val loss: 4.4004\n",
      "Epoch 807 | Train loss: 4.1407 | Val loss: 4.4172\n",
      "Epoch 808 | Train loss: 4.1135 | Val loss: 4.4825\n",
      "Epoch 809 | Train loss: 4.0863 | Val loss: 4.4104\n",
      "Epoch 810 | Train loss: 4.1286 | Val loss: 4.4272\n",
      "Epoch 811 | Train loss: 4.0971 | Val loss: 4.4180\n",
      "Epoch 812 | Train loss: 4.1479 | Val loss: 4.4200\n",
      "Epoch 813 | Train loss: 4.1710 | Val loss: 4.3997\n",
      "Epoch 814 | Train loss: 4.1013 | Val loss: 4.4196\n",
      "Epoch 815 | Train loss: 4.1338 | Val loss: 4.3909\n",
      "Epoch 816 | Train loss: 4.1216 | Val loss: 4.4343\n",
      "Epoch 817 | Train loss: 4.1001 | Val loss: 4.4401\n",
      "Epoch 818 | Train loss: 4.1490 | Val loss: 4.3944\n",
      "Epoch 819 | Train loss: 4.1300 | Val loss: 4.4238\n",
      "Epoch 820 | Train loss: 4.1446 | Val loss: 4.3606\n",
      "Epoch 821 | Train loss: 4.1209 | Val loss: 4.4276\n",
      "Epoch 822 | Train loss: 4.1289 | Val loss: 4.3729\n",
      "Epoch 823 | Train loss: 4.1230 | Val loss: 4.4235\n",
      "Epoch 824 | Train loss: 4.1203 | Val loss: 4.3833\n",
      "Epoch 825 | Train loss: 4.1012 | Val loss: 4.3884\n",
      "Epoch 826 | Train loss: 4.1172 | Val loss: 4.4199\n",
      "Epoch 827 | Train loss: 4.1087 | Val loss: 4.3740\n",
      "Epoch 828 | Train loss: 4.1311 | Val loss: 4.4109\n",
      "Epoch 829 | Train loss: 4.1197 | Val loss: 4.4581\n",
      "Epoch 830 | Train loss: 4.1744 | Val loss: 4.3938\n",
      "Epoch 831 | Train loss: 4.1549 | Val loss: 4.3974\n",
      "Epoch 832 | Train loss: 4.1481 | Val loss: 4.3724\n",
      "Epoch 833 | Train loss: 4.1193 | Val loss: 4.3627\n",
      "Epoch 834 | Train loss: 4.1123 | Val loss: 4.3407\n",
      "Epoch 835 | Train loss: 4.1228 | Val loss: 4.4275\n",
      "Epoch 836 | Train loss: 4.1668 | Val loss: 4.4018\n",
      "Epoch 837 | Train loss: 4.1369 | Val loss: 4.4359\n",
      "Epoch 838 | Train loss: 4.0758 | Val loss: 4.4234\n",
      "Epoch 839 | Train loss: 4.0768 | Val loss: 4.3983\n",
      "Epoch 840 | Train loss: 4.1731 | Val loss: 4.4286\n",
      "Epoch 841 | Train loss: 4.1278 | Val loss: 4.3946\n",
      "Epoch 842 | Train loss: 4.1474 | Val loss: 4.3853\n",
      "Epoch 843 | Train loss: 4.1352 | Val loss: 4.4269\n",
      "Epoch 844 | Train loss: 4.1215 | Val loss: 4.4260\n",
      "Epoch 845 | Train loss: 4.1243 | Val loss: 4.3896\n",
      "Epoch 846 | Train loss: 4.1046 | Val loss: 4.3423\n",
      "Epoch 847 | Train loss: 4.1152 | Val loss: 4.4263\n",
      "Epoch 848 | Train loss: 4.1219 | Val loss: 4.4420\n",
      "Epoch 849 | Train loss: 4.1365 | Val loss: 4.4202\n",
      "Epoch 850 | Train loss: 4.1242 | Val loss: 4.4155\n",
      "Epoch 851 | Train loss: 4.1159 | Val loss: 4.4214\n",
      "Epoch 852 | Train loss: 4.1113 | Val loss: 4.3387\n",
      "Epoch 853 | Train loss: 4.1051 | Val loss: 4.4142\n",
      "Epoch 854 | Train loss: 4.1437 | Val loss: 4.4345\n",
      "Epoch 855 | Train loss: 4.0687 | Val loss: 4.4115\n",
      "Epoch 856 | Train loss: 4.1310 | Val loss: 4.4201\n",
      "Epoch 857 | Train loss: 4.0937 | Val loss: 4.3873\n",
      "Epoch 858 | Train loss: 4.1089 | Val loss: 4.4135\n",
      "Epoch 859 | Train loss: 4.0805 | Val loss: 4.4204\n",
      "Epoch 860 | Train loss: 4.1349 | Val loss: 4.4034\n",
      "Epoch 861 | Train loss: 4.1176 | Val loss: 4.3751\n",
      "Epoch 862 | Train loss: 4.0976 | Val loss: 4.4964\n",
      "Epoch 863 | Train loss: 4.0895 | Val loss: 4.4631\n",
      "Epoch 864 | Train loss: 4.1029 | Val loss: 4.3598\n",
      "Epoch 865 | Train loss: 4.1108 | Val loss: 4.4022\n",
      "Epoch 866 | Train loss: 4.1109 | Val loss: 4.4368\n",
      "Epoch 867 | Train loss: 4.0884 | Val loss: 4.4313\n",
      "Epoch 868 | Train loss: 4.0512 | Val loss: 4.4120\n",
      "Epoch 869 | Train loss: 4.1261 | Val loss: 4.3665\n",
      "Epoch 870 | Train loss: 4.1467 | Val loss: 4.4133\n",
      "Epoch 871 | Train loss: 4.1252 | Val loss: 4.3998\n",
      "Epoch 872 | Train loss: 4.0969 | Val loss: 4.4306\n",
      "Epoch 873 | Train loss: 4.0709 | Val loss: 4.4578\n",
      "Epoch 874 | Train loss: 4.0955 | Val loss: 4.3883\n",
      "Epoch 875 | Train loss: 4.1184 | Val loss: 4.4139\n",
      "Epoch 876 | Train loss: 4.0796 | Val loss: 4.3543\n",
      "Epoch 877 | Train loss: 4.0882 | Val loss: 4.4481\n",
      "Epoch 878 | Train loss: 4.1094 | Val loss: 4.3643\n",
      "Epoch 879 | Train loss: 4.0919 | Val loss: 4.4044\n",
      "Epoch 880 | Train loss: 4.0851 | Val loss: 4.3895\n",
      "Epoch 881 | Train loss: 4.0926 | Val loss: 4.3949\n",
      "Epoch 882 | Train loss: 4.0792 | Val loss: 4.4130\n",
      "Epoch 883 | Train loss: 4.1210 | Val loss: 4.3443\n",
      "Epoch 884 | Train loss: 4.1063 | Val loss: 4.4024\n",
      "Epoch 885 | Train loss: 4.0149 | Val loss: 4.3781\n",
      "Epoch 886 | Train loss: 4.1342 | Val loss: 4.3898\n",
      "Epoch 887 | Train loss: 4.0842 | Val loss: 4.3939\n",
      "Epoch 888 | Train loss: 4.1090 | Val loss: 4.4046\n",
      "Epoch 889 | Train loss: 4.0982 | Val loss: 4.3676\n",
      "Epoch 890 | Train loss: 4.0678 | Val loss: 4.3640\n",
      "Epoch 891 | Train loss: 4.0559 | Val loss: 4.4365\n",
      "Epoch 892 | Train loss: 4.0703 | Val loss: 4.4053\n",
      "Epoch 893 | Train loss: 4.0649 | Val loss: 4.3929\n",
      "Epoch 894 | Train loss: 4.0384 | Val loss: 4.4186\n",
      "Epoch 895 | Train loss: 4.1264 | Val loss: 4.4096\n",
      "Epoch 896 | Train loss: 4.0871 | Val loss: 4.3871\n",
      "Epoch 897 | Train loss: 4.1287 | Val loss: 4.4046\n",
      "Epoch 898 | Train loss: 4.0121 | Val loss: 4.3985\n",
      "Epoch 899 | Train loss: 4.1117 | Val loss: 4.4035\n",
      "Epoch 900 | Train loss: 4.1027 | Val loss: 4.4160\n",
      "Epoch 901 | Train loss: 4.0834 | Val loss: 4.3993\n",
      "Epoch 902 | Train loss: 4.0636 | Val loss: 4.4227\n",
      "Epoch 903 | Train loss: 4.1052 | Val loss: 4.3941\n",
      "Epoch 904 | Train loss: 4.0834 | Val loss: 4.3386\n",
      "Epoch 905 | Train loss: 4.0755 | Val loss: 4.4140\n",
      "Epoch 906 | Train loss: 4.0739 | Val loss: 4.3788\n",
      "Epoch 907 | Train loss: 4.0834 | Val loss: 4.4140\n",
      "Epoch 908 | Train loss: 4.0639 | Val loss: 4.3522\n",
      "Epoch 909 | Train loss: 4.1180 | Val loss: 4.3786\n",
      "Epoch 910 | Train loss: 4.1326 | Val loss: 4.3419\n",
      "Epoch 911 | Train loss: 4.0701 | Val loss: 4.3468\n",
      "Epoch 912 | Train loss: 4.0982 | Val loss: 4.3461\n",
      "Epoch 913 | Train loss: 4.0606 | Val loss: 4.3772\n",
      "Epoch 914 | Train loss: 4.0752 | Val loss: 4.4724\n",
      "Epoch 915 | Train loss: 4.0798 | Val loss: 4.3798\n",
      "Epoch 916 | Train loss: 4.0935 | Val loss: 4.4356\n",
      "Epoch 917 | Train loss: 4.0819 | Val loss: 4.3699\n",
      "Epoch 918 | Train loss: 4.1107 | Val loss: 4.3763\n",
      "Epoch 919 | Train loss: 4.0904 | Val loss: 4.4135\n",
      "Epoch 920 | Train loss: 4.1112 | Val loss: 4.4061\n",
      "Epoch 921 | Train loss: 4.0749 | Val loss: 4.3654\n",
      "Epoch 922 | Train loss: 4.0972 | Val loss: 4.4196\n",
      "Epoch 923 | Train loss: 4.1163 | Val loss: 4.3577\n",
      "Epoch 924 | Train loss: 4.1266 | Val loss: 4.4083\n",
      "Epoch 925 | Train loss: 4.1085 | Val loss: 4.4073\n",
      "Epoch 926 | Train loss: 4.0888 | Val loss: 4.3844\n",
      "Epoch 927 | Train loss: 4.1099 | Val loss: 4.3521\n",
      "Epoch 928 | Train loss: 4.0879 | Val loss: 4.3811\n",
      "Epoch 929 | Train loss: 4.1358 | Val loss: 4.4131\n",
      "Epoch 930 | Train loss: 4.0641 | Val loss: 4.4165\n",
      "Epoch 931 | Train loss: 4.0737 | Val loss: 4.3990\n",
      "Epoch 932 | Train loss: 4.0662 | Val loss: 4.3595\n",
      "Epoch 933 | Train loss: 4.0873 | Val loss: 4.3706\n",
      "Epoch 934 | Train loss: 4.0488 | Val loss: 4.3326\n",
      "Epoch 935 | Train loss: 4.0243 | Val loss: 4.3573\n",
      "Epoch 936 | Train loss: 4.0896 | Val loss: 4.3729\n",
      "Epoch 937 | Train loss: 4.1067 | Val loss: 4.3845\n",
      "Epoch 938 | Train loss: 4.1088 | Val loss: 4.3397\n",
      "Epoch 939 | Train loss: 4.0695 | Val loss: 4.3088\n",
      "Epoch 940 | Train loss: 4.0499 | Val loss: 4.2825\n",
      "Epoch 941 | Train loss: 4.0760 | Val loss: 4.3707\n",
      "Epoch 942 | Train loss: 4.0620 | Val loss: 4.3880\n",
      "Epoch 943 | Train loss: 4.1240 | Val loss: 4.3764\n",
      "Epoch 944 | Train loss: 4.0435 | Val loss: 4.4199\n",
      "Epoch 945 | Train loss: 4.0216 | Val loss: 4.4697\n",
      "Epoch 946 | Train loss: 4.1216 | Val loss: 4.4048\n",
      "Epoch 947 | Train loss: 4.0268 | Val loss: 4.3758\n",
      "Epoch 948 | Train loss: 4.0354 | Val loss: 4.3291\n",
      "Epoch 949 | Train loss: 4.0848 | Val loss: 4.3770\n",
      "Epoch 950 | Train loss: 4.0354 | Val loss: 4.3815\n",
      "Epoch 951 | Train loss: 4.1056 | Val loss: 4.3676\n",
      "Epoch 952 | Train loss: 4.0676 | Val loss: 4.3364\n",
      "Epoch 953 | Train loss: 4.0907 | Val loss: 4.4002\n",
      "Epoch 954 | Train loss: 4.0526 | Val loss: 4.3694\n",
      "Epoch 955 | Train loss: 4.0834 | Val loss: 4.3394\n",
      "Epoch 956 | Train loss: 4.0741 | Val loss: 4.3870\n",
      "Epoch 957 | Train loss: 4.0690 | Val loss: 4.3884\n",
      "Epoch 958 | Train loss: 4.0331 | Val loss: 4.4083\n",
      "Epoch 959 | Train loss: 4.0553 | Val loss: 4.3195\n",
      "Epoch 960 | Train loss: 4.0910 | Val loss: 4.3966\n",
      "Epoch 961 | Train loss: 4.0479 | Val loss: 4.3684\n",
      "Epoch 962 | Train loss: 4.0761 | Val loss: 4.3759\n",
      "Epoch 963 | Train loss: 4.0783 | Val loss: 4.3443\n",
      "Epoch 964 | Train loss: 4.0768 | Val loss: 4.3604\n",
      "Epoch 965 | Train loss: 4.0161 | Val loss: 4.3703\n",
      "Epoch 966 | Train loss: 4.0688 | Val loss: 4.4165\n",
      "Epoch 967 | Train loss: 4.0556 | Val loss: 4.3775\n",
      "Epoch 968 | Train loss: 4.0608 | Val loss: 4.3697\n",
      "Epoch 969 | Train loss: 4.0793 | Val loss: 4.3255\n",
      "Epoch 970 | Train loss: 4.0446 | Val loss: 4.3717\n",
      "Epoch 971 | Train loss: 4.0615 | Val loss: 4.3746\n",
      "Epoch 972 | Train loss: 4.0623 | Val loss: 4.3838\n",
      "Epoch 973 | Train loss: 4.0636 | Val loss: 4.3520\n",
      "Epoch 974 | Train loss: 4.0639 | Val loss: 4.3712\n",
      "Epoch 975 | Train loss: 3.9957 | Val loss: 4.3604\n",
      "Epoch 976 | Train loss: 4.0985 | Val loss: 4.3582\n",
      "Epoch 977 | Train loss: 4.0089 | Val loss: 4.3359\n",
      "Epoch 978 | Train loss: 4.0343 | Val loss: 4.3455\n",
      "Epoch 979 | Train loss: 4.0450 | Val loss: 4.3804\n",
      "Epoch 980 | Train loss: 4.0212 | Val loss: 4.3104\n",
      "Epoch 981 | Train loss: 4.0345 | Val loss: 4.3408\n",
      "Epoch 982 | Train loss: 4.0051 | Val loss: 4.3463\n",
      "Epoch 983 | Train loss: 4.0705 | Val loss: 4.3876\n",
      "Epoch 984 | Train loss: 4.0352 | Val loss: 4.3252\n",
      "Epoch 985 | Train loss: 4.0830 | Val loss: 4.3410\n",
      "Epoch 986 | Train loss: 4.0637 | Val loss: 4.3365\n",
      "Epoch 987 | Train loss: 4.1278 | Val loss: 4.3592\n",
      "Epoch 988 | Train loss: 4.0336 | Val loss: 4.3500\n",
      "Epoch 989 | Train loss: 4.0249 | Val loss: 4.3767\n",
      "Epoch 990 | Train loss: 4.0753 | Val loss: 4.3950\n",
      "Epoch 991 | Train loss: 4.0440 | Val loss: 4.3564\n",
      "Epoch 992 | Train loss: 4.0422 | Val loss: 4.3524\n",
      "Epoch 993 | Train loss: 4.0859 | Val loss: 4.3864\n",
      "Epoch 994 | Train loss: 4.0196 | Val loss: 4.4078\n",
      "Epoch 995 | Train loss: 4.0670 | Val loss: 4.4278\n",
      "Epoch 996 | Train loss: 4.0451 | Val loss: 4.4090\n",
      "Epoch 997 | Train loss: 4.0337 | Val loss: 4.3484\n",
      "Epoch 998 | Train loss: 4.0775 | Val loss: 4.3724\n",
      "Epoch 999 | Train loss: 4.0634 | Val loss: 4.4151\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "model = Transformer_Decoder(vocab_size, num_layers, num_heads, embed_dim, max_input_length, input_dropout).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  x, y = get_batch('train', device=device, input_length=max_input_length, batch_size=batch_size)\n",
    "  y_logits = model(x)\n",
    "    \n",
    "  B, T, C = y_logits.shape\n",
    "  loss = loss_fn(y_logits.reshape(B * T, C), y.reshape(B * T))\n",
    "  train_loss += loss.item()\n",
    "\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if epoch % eval_interval == 0 or epoch == epochs-1:\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      x, y = get_batch('val', device=device, input_length=max_input_length, batch_size=batch_size)\n",
    "\n",
    "      y_logits = model(x)\n",
    "    \n",
    "      B, T, C = y_logits.shape\n",
    "      loss = loss_fn(y_logits.reshape(B * T, C), y.reshape(B * T))\n",
    "      val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")\n",
    "    \n",
    "    torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': train_loss,\n",
    "    }, 'model/shakespeare_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddebbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/shakespeare_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333401a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c11629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.6359\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "  x, y = get_batch('test', device=device, input_length=max_input_length, batch_size=batch_size)\n",
    "\n",
    "  y_logits = model(x)\n",
    "    \n",
    "  B, T, C = y_logits.shape\n",
    "  loss = loss_fn(y_logits.reshape(B * T, C), y.reshape(B * T))\n",
    "  test_loss += loss.item()\n",
    "    \n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de459d",
   "metadata": {},
   "source": [
    "## Generate Shakespeare-like text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a860883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), torch.Size([1, 257]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer_Decoder(vocab_size, num_layers, num_heads, embed_dim, max_input_length, input_dropout).to(device)\n",
    "model.load_state_dict(torch.load('model/shakespeare_model_weights.pth'))\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "output = model.generate(context, tokenizer.special_tokens['<eos>'], max_new_tokens=256)\n",
    "context.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ccc87715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000up mine own:\n",
      "Fell, the helfter per, one ashower.\n",
      "Now, the pted lie my and pike of your noble t,\n",
      "And heaven or lover a king.\n",
      "\n",
      "DORDORTHUMNORTHASTANNwot to himself besces inter:\n",
      "And not! theirs.\n",
      "\n",
      "KING RICHARD II:\n",
      "Marence.\n",
      "\n",
      "D:\n",
      "And then be etion I toorper dand:\n",
      "Bosusback you;\n",
      "Andier, and meanswer, I'lly, that wayill?\n",
      "\n",
      "WEDWARD IV:\n",
      "Barwick, I hamour plaguster, no man forbece, fin their of ht, ifesed I shall have meetchilts to this g our Now V:\n",
      "Warwick, that stit!\n",
      "I have had buts it my in solge. Fa mays ones,\n",
      "Since fe, to seas?\n",
      "\n",
      "HENRY Brazisso.\n",
      "\n",
      "CLAND:\n",
      "GLOUCESTER:\n",
      "It helree\n",
      "Thanest has now thou nolow, held, with thy seenough spossild hat's faafforth\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0].cpu().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
