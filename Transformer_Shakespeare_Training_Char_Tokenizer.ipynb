{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "067ac405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from Transformer import Transformer_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fdff49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "  shakespeare_text = f.read()\n",
    "\n",
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0675ee",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a275d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(shakespeare_text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers and vice versa\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]           # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb042fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_tokenized = encode(shakespeare_text)\n",
    "shakespeare_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d7b28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(shakespeare_tokenized[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60e5743f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_tokenized_tensor = torch.tensor(shakespeare_tokenized, dtype=torch.long)\n",
    "\n",
    "total_length = len(shakespeare_tokenized_tensor)\n",
    "\n",
    "train_data = shakespeare_tokenized_tensor[:total_length * 90 // 100]\n",
    "val_data = shakespeare_tokenized_tensor[total_length * 90 // 100:]\n",
    "\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a8bfe",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ce99be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256]), torch.Size([64, 256]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(split, device, input_length=256, batch_size=64):\n",
    "    data = train_data\n",
    "    if split == 'val':\n",
    "        data = val_data\n",
    "    \n",
    "    ix = torch.randint(len(data) - input_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+input_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+input_length+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train', 'cpu', input_length=256, batch_size=64)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c375c42",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c2f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "max_input_length = 256\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 6\n",
    "embed_dim = 384\n",
    "input_dropout = 0.2\n",
    "\n",
    "lr = 0.0003\n",
    "epochs = 1000\n",
    "eval_interval = 1\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f14d2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb791b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss: 4.4013 | Val loss: 3.9155\n",
      "Epoch 1 | Train loss: 3.7516 | Val loss: 3.4881\n",
      "Epoch 2 | Train loss: 3.5366 | Val loss: 3.5311\n",
      "Epoch 3 | Train loss: 3.5521 | Val loss: 3.4301\n",
      "Epoch 4 | Train loss: 3.4187 | Val loss: 3.3043\n",
      "Epoch 5 | Train loss: 3.3228 | Val loss: 3.2782\n",
      "Epoch 6 | Train loss: 3.2462 | Val loss: 3.3088\n",
      "Epoch 7 | Train loss: 3.2902 | Val loss: 3.3433\n",
      "Epoch 8 | Train loss: 3.2938 | Val loss: 3.2300\n",
      "Epoch 9 | Train loss: 3.2563 | Val loss: 3.1969\n",
      "Epoch 10 | Train loss: 3.2076 | Val loss: 3.1429\n",
      "Epoch 11 | Train loss: 3.1441 | Val loss: 3.1190\n",
      "Epoch 12 | Train loss: 3.1279 | Val loss: 3.0718\n",
      "Epoch 13 | Train loss: 3.1251 | Val loss: 3.0477\n",
      "Epoch 14 | Train loss: 3.0532 | Val loss: 2.9681\n",
      "Epoch 15 | Train loss: 2.9792 | Val loss: 2.9208\n",
      "Epoch 16 | Train loss: 2.9543 | Val loss: 2.9221\n",
      "Epoch 17 | Train loss: 2.9004 | Val loss: 2.9486\n",
      "Epoch 18 | Train loss: 2.9213 | Val loss: 2.8706\n",
      "Epoch 19 | Train loss: 2.8821 | Val loss: 2.8542\n",
      "Epoch 20 | Train loss: 2.8387 | Val loss: 2.8306\n",
      "Epoch 21 | Train loss: 2.8437 | Val loss: 2.8096\n",
      "Epoch 22 | Train loss: 2.8394 | Val loss: 2.8176\n",
      "Epoch 23 | Train loss: 2.7956 | Val loss: 2.7759\n",
      "Epoch 24 | Train loss: 2.7605 | Val loss: 2.7914\n",
      "Epoch 25 | Train loss: 2.7537 | Val loss: 2.7309\n",
      "Epoch 26 | Train loss: 2.7524 | Val loss: 2.7116\n",
      "Epoch 27 | Train loss: 2.7335 | Val loss: 2.7181\n",
      "Epoch 28 | Train loss: 2.7431 | Val loss: 2.7142\n",
      "Epoch 29 | Train loss: 2.6852 | Val loss: 2.6600\n",
      "Epoch 30 | Train loss: 2.7010 | Val loss: 2.6603\n",
      "Epoch 31 | Train loss: 2.6710 | Val loss: 2.6252\n",
      "Epoch 32 | Train loss: 2.6726 | Val loss: 2.6354\n",
      "Epoch 33 | Train loss: 2.6745 | Val loss: 2.6198\n",
      "Epoch 34 | Train loss: 2.6284 | Val loss: 2.5982\n",
      "Epoch 35 | Train loss: 2.6168 | Val loss: 2.6129\n",
      "Epoch 36 | Train loss: 2.6422 | Val loss: 2.5936\n",
      "Epoch 37 | Train loss: 2.6071 | Val loss: 2.5858\n",
      "Epoch 38 | Train loss: 2.6092 | Val loss: 2.5795\n",
      "Epoch 39 | Train loss: 2.5892 | Val loss: 2.5696\n",
      "Epoch 40 | Train loss: 2.6080 | Val loss: 2.5824\n",
      "Epoch 41 | Train loss: 2.5676 | Val loss: 2.5910\n",
      "Epoch 42 | Train loss: 2.5963 | Val loss: 2.5587\n",
      "Epoch 43 | Train loss: 2.5759 | Val loss: 2.5693\n",
      "Epoch 44 | Train loss: 2.5728 | Val loss: 2.5534\n",
      "Epoch 45 | Train loss: 2.5700 | Val loss: 2.5637\n",
      "Epoch 46 | Train loss: 2.5617 | Val loss: 2.5424\n",
      "Epoch 47 | Train loss: 2.5800 | Val loss: 2.5529\n",
      "Epoch 48 | Train loss: 2.5688 | Val loss: 2.5566\n",
      "Epoch 49 | Train loss: 2.5810 | Val loss: 2.5109\n",
      "Epoch 50 | Train loss: 2.5487 | Val loss: 2.5253\n",
      "Epoch 51 | Train loss: 2.5383 | Val loss: 2.5138\n",
      "Epoch 52 | Train loss: 2.5643 | Val loss: 2.5303\n",
      "Epoch 53 | Train loss: 2.5672 | Val loss: 2.5385\n",
      "Epoch 54 | Train loss: 2.5600 | Val loss: 2.4874\n",
      "Epoch 55 | Train loss: 2.5594 | Val loss: 2.5366\n",
      "Epoch 56 | Train loss: 2.5374 | Val loss: 2.5047\n",
      "Epoch 57 | Train loss: 2.5426 | Val loss: 2.5200\n",
      "Epoch 58 | Train loss: 2.5454 | Val loss: 2.4876\n",
      "Epoch 59 | Train loss: 2.5377 | Val loss: 2.5150\n",
      "Epoch 60 | Train loss: 2.5621 | Val loss: 2.5144\n",
      "Epoch 61 | Train loss: 2.5266 | Val loss: 2.5049\n",
      "Epoch 62 | Train loss: 2.5172 | Val loss: 2.4652\n",
      "Epoch 63 | Train loss: 2.5514 | Val loss: 2.5127\n",
      "Epoch 64 | Train loss: 2.5347 | Val loss: 2.5035\n",
      "Epoch 65 | Train loss: 2.5379 | Val loss: 2.4954\n",
      "Epoch 66 | Train loss: 2.5153 | Val loss: 2.5203\n",
      "Epoch 67 | Train loss: 2.5306 | Val loss: 2.4966\n",
      "Epoch 68 | Train loss: 2.5411 | Val loss: 2.5279\n",
      "Epoch 69 | Train loss: 2.5250 | Val loss: 2.5018\n",
      "Epoch 70 | Train loss: 2.5224 | Val loss: 2.4922\n",
      "Epoch 71 | Train loss: 2.5202 | Val loss: 2.5045\n",
      "Epoch 72 | Train loss: 2.5146 | Val loss: 2.4919\n",
      "Epoch 73 | Train loss: 2.5477 | Val loss: 2.4859\n",
      "Epoch 74 | Train loss: 2.5163 | Val loss: 2.4913\n",
      "Epoch 75 | Train loss: 2.5180 | Val loss: 2.4813\n",
      "Epoch 76 | Train loss: 2.5336 | Val loss: 2.4874\n",
      "Epoch 77 | Train loss: 2.5196 | Val loss: 2.4843\n",
      "Epoch 78 | Train loss: 2.5306 | Val loss: 2.5003\n",
      "Epoch 79 | Train loss: 2.5214 | Val loss: 2.5020\n",
      "Epoch 80 | Train loss: 2.5282 | Val loss: 2.4811\n",
      "Epoch 81 | Train loss: 2.5050 | Val loss: 2.4917\n",
      "Epoch 82 | Train loss: 2.5153 | Val loss: 2.4959\n",
      "Epoch 83 | Train loss: 2.5093 | Val loss: 2.4919\n",
      "Epoch 84 | Train loss: 2.5110 | Val loss: 2.4841\n",
      "Epoch 85 | Train loss: 2.5137 | Val loss: 2.4980\n",
      "Epoch 86 | Train loss: 2.5198 | Val loss: 2.4804\n",
      "Epoch 87 | Train loss: 2.5045 | Val loss: 2.4775\n",
      "Epoch 88 | Train loss: 2.5147 | Val loss: 2.4766\n",
      "Epoch 89 | Train loss: 2.5161 | Val loss: 2.4971\n",
      "Epoch 90 | Train loss: 2.5140 | Val loss: 2.4815\n",
      "Epoch 91 | Train loss: 2.4991 | Val loss: 2.4846\n",
      "Epoch 92 | Train loss: 2.5021 | Val loss: 2.4949\n",
      "Epoch 93 | Train loss: 2.4829 | Val loss: 2.4588\n",
      "Epoch 94 | Train loss: 2.5025 | Val loss: 2.4833\n",
      "Epoch 95 | Train loss: 2.4902 | Val loss: 2.4944\n",
      "Epoch 96 | Train loss: 2.4869 | Val loss: 2.4939\n",
      "Epoch 97 | Train loss: 2.4965 | Val loss: 2.4896\n",
      "Epoch 98 | Train loss: 2.5005 | Val loss: 2.5171\n",
      "Epoch 99 | Train loss: 2.4803 | Val loss: 2.5040\n",
      "Epoch 100 | Train loss: 2.4860 | Val loss: 2.4921\n",
      "Epoch 101 | Train loss: 2.4902 | Val loss: 2.4914\n",
      "Epoch 102 | Train loss: 2.5000 | Val loss: 2.4900\n",
      "Epoch 103 | Train loss: 2.5006 | Val loss: 2.4860\n",
      "Epoch 104 | Train loss: 2.4866 | Val loss: 2.4699\n",
      "Epoch 105 | Train loss: 2.4950 | Val loss: 2.4676\n",
      "Epoch 106 | Train loss: 2.4874 | Val loss: 2.4717\n",
      "Epoch 107 | Train loss: 2.4655 | Val loss: 2.4740\n",
      "Epoch 108 | Train loss: 2.4870 | Val loss: 2.4720\n",
      "Epoch 109 | Train loss: 2.4945 | Val loss: 2.5060\n",
      "Epoch 110 | Train loss: 2.4893 | Val loss: 2.4793\n",
      "Epoch 111 | Train loss: 2.4850 | Val loss: 2.4626\n",
      "Epoch 112 | Train loss: 2.4660 | Val loss: 2.4959\n",
      "Epoch 113 | Train loss: 2.5149 | Val loss: 2.4692\n",
      "Epoch 114 | Train loss: 2.4803 | Val loss: 2.4694\n",
      "Epoch 115 | Train loss: 2.4732 | Val loss: 2.4545\n",
      "Epoch 116 | Train loss: 2.4622 | Val loss: 2.4606\n",
      "Epoch 117 | Train loss: 2.4933 | Val loss: 2.4520\n",
      "Epoch 118 | Train loss: 2.4675 | Val loss: 2.4747\n",
      "Epoch 119 | Train loss: 2.4704 | Val loss: 2.4955\n",
      "Epoch 120 | Train loss: 2.4692 | Val loss: 2.4779\n",
      "Epoch 121 | Train loss: 2.4702 | Val loss: 2.4573\n",
      "Epoch 122 | Train loss: 2.4831 | Val loss: 2.4473\n",
      "Epoch 123 | Train loss: 2.4714 | Val loss: 2.4578\n",
      "Epoch 124 | Train loss: 2.4765 | Val loss: 2.4423\n",
      "Epoch 125 | Train loss: 2.4629 | Val loss: 2.4532\n",
      "Epoch 126 | Train loss: 2.4698 | Val loss: 2.4531\n",
      "Epoch 127 | Train loss: 2.4638 | Val loss: 2.4509\n",
      "Epoch 128 | Train loss: 2.4688 | Val loss: 2.4341\n",
      "Epoch 129 | Train loss: 2.4597 | Val loss: 2.4323\n",
      "Epoch 130 | Train loss: 2.4656 | Val loss: 2.4377\n",
      "Epoch 131 | Train loss: 2.4783 | Val loss: 2.4565\n",
      "Epoch 132 | Train loss: 2.4560 | Val loss: 2.4431\n",
      "Epoch 133 | Train loss: 2.4705 | Val loss: 2.4726\n",
      "Epoch 134 | Train loss: 2.4597 | Val loss: 2.4304\n",
      "Epoch 135 | Train loss: 2.4666 | Val loss: 2.4735\n",
      "Epoch 136 | Train loss: 2.4669 | Val loss: 2.4463\n",
      "Epoch 137 | Train loss: 2.4430 | Val loss: 2.4502\n",
      "Epoch 138 | Train loss: 2.4684 | Val loss: 2.4588\n",
      "Epoch 139 | Train loss: 2.4684 | Val loss: 2.4348\n",
      "Epoch 140 | Train loss: 2.4431 | Val loss: 2.4305\n",
      "Epoch 141 | Train loss: 2.4332 | Val loss: 2.4471\n",
      "Epoch 142 | Train loss: 2.4461 | Val loss: 2.4628\n",
      "Epoch 143 | Train loss: 2.4478 | Val loss: 2.4394\n",
      "Epoch 144 | Train loss: 2.4535 | Val loss: 2.4322\n",
      "Epoch 145 | Train loss: 2.4458 | Val loss: 2.4310\n",
      "Epoch 146 | Train loss: 2.4745 | Val loss: 2.4589\n",
      "Epoch 147 | Train loss: 2.4563 | Val loss: 2.4445\n",
      "Epoch 148 | Train loss: 2.4561 | Val loss: 2.4400\n",
      "Epoch 149 | Train loss: 2.4369 | Val loss: 2.4409\n",
      "Epoch 150 | Train loss: 2.4293 | Val loss: 2.4250\n",
      "Epoch 151 | Train loss: 2.4483 | Val loss: 2.4112\n",
      "Epoch 152 | Train loss: 2.4354 | Val loss: 2.4421\n",
      "Epoch 153 | Train loss: 2.4434 | Val loss: 2.4120\n",
      "Epoch 154 | Train loss: 2.4299 | Val loss: 2.4145\n",
      "Epoch 155 | Train loss: 2.4385 | Val loss: 2.4154\n",
      "Epoch 156 | Train loss: 2.4288 | Val loss: 2.4319\n",
      "Epoch 157 | Train loss: 2.4319 | Val loss: 2.4318\n",
      "Epoch 158 | Train loss: 2.4414 | Val loss: 2.4204\n",
      "Epoch 159 | Train loss: 2.4329 | Val loss: 2.4542\n",
      "Epoch 160 | Train loss: 2.4412 | Val loss: 2.4147\n",
      "Epoch 161 | Train loss: 2.4616 | Val loss: 2.4341\n",
      "Epoch 162 | Train loss: 2.4273 | Val loss: 2.4063\n",
      "Epoch 163 | Train loss: 2.4367 | Val loss: 2.4196\n",
      "Epoch 164 | Train loss: 2.4402 | Val loss: 2.4134\n",
      "Epoch 165 | Train loss: 2.4224 | Val loss: 2.3994\n",
      "Epoch 166 | Train loss: 2.4210 | Val loss: 2.4369\n",
      "Epoch 167 | Train loss: 2.4185 | Val loss: 2.4078\n",
      "Epoch 168 | Train loss: 2.4435 | Val loss: 2.4298\n",
      "Epoch 169 | Train loss: 2.4449 | Val loss: 2.4129\n",
      "Epoch 170 | Train loss: 2.4597 | Val loss: 2.4015\n",
      "Epoch 171 | Train loss: 2.4120 | Val loss: 2.3799\n",
      "Epoch 172 | Train loss: 2.4210 | Val loss: 2.4123\n",
      "Epoch 173 | Train loss: 2.4376 | Val loss: 2.4236\n",
      "Epoch 174 | Train loss: 2.4120 | Val loss: 2.4020\n",
      "Epoch 175 | Train loss: 2.4395 | Val loss: 2.4198\n",
      "Epoch 176 | Train loss: 2.4056 | Val loss: 2.4142\n",
      "Epoch 177 | Train loss: 2.4268 | Val loss: 2.3951\n",
      "Epoch 178 | Train loss: 2.4287 | Val loss: 2.3754\n",
      "Epoch 179 | Train loss: 2.4180 | Val loss: 2.3876\n",
      "Epoch 180 | Train loss: 2.3942 | Val loss: 2.4088\n",
      "Epoch 181 | Train loss: 2.4038 | Val loss: 2.3844\n",
      "Epoch 182 | Train loss: 2.4053 | Val loss: 2.3915\n",
      "Epoch 183 | Train loss: 2.3962 | Val loss: 2.4142\n",
      "Epoch 184 | Train loss: 2.4104 | Val loss: 2.4082\n",
      "Epoch 185 | Train loss: 2.4164 | Val loss: 2.3723\n",
      "Epoch 186 | Train loss: 2.4050 | Val loss: 2.3770\n",
      "Epoch 187 | Train loss: 2.4159 | Val loss: 2.3929\n",
      "Epoch 188 | Train loss: 2.4021 | Val loss: 2.4215\n",
      "Epoch 189 | Train loss: 2.4004 | Val loss: 2.3844\n",
      "Epoch 190 | Train loss: 2.3945 | Val loss: 2.3736\n",
      "Epoch 191 | Train loss: 2.4117 | Val loss: 2.4077\n",
      "Epoch 192 | Train loss: 2.4234 | Val loss: 2.3867\n",
      "Epoch 193 | Train loss: 2.3988 | Val loss: 2.3811\n",
      "Epoch 194 | Train loss: 2.3873 | Val loss: 2.3700\n",
      "Epoch 195 | Train loss: 2.4140 | Val loss: 2.4074\n",
      "Epoch 196 | Train loss: 2.4139 | Val loss: 2.3944\n",
      "Epoch 197 | Train loss: 2.4101 | Val loss: 2.4108\n",
      "Epoch 198 | Train loss: 2.3981 | Val loss: 2.3913\n",
      "Epoch 199 | Train loss: 2.3902 | Val loss: 2.3614\n",
      "Epoch 200 | Train loss: 2.3824 | Val loss: 2.3886\n",
      "Epoch 201 | Train loss: 2.3964 | Val loss: 2.3709\n",
      "Epoch 202 | Train loss: 2.4121 | Val loss: 2.3758\n",
      "Epoch 203 | Train loss: 2.3962 | Val loss: 2.3950\n",
      "Epoch 204 | Train loss: 2.3894 | Val loss: 2.3628\n",
      "Epoch 205 | Train loss: 2.3620 | Val loss: 2.3674\n",
      "Epoch 206 | Train loss: 2.3833 | Val loss: 2.3775\n",
      "Epoch 207 | Train loss: 2.3999 | Val loss: 2.3756\n",
      "Epoch 208 | Train loss: 2.3939 | Val loss: 2.3664\n",
      "Epoch 209 | Train loss: 2.4040 | Val loss: 2.3513\n",
      "Epoch 210 | Train loss: 2.3868 | Val loss: 2.3744\n",
      "Epoch 211 | Train loss: 2.3705 | Val loss: 2.3417\n",
      "Epoch 212 | Train loss: 2.3627 | Val loss: 2.3317\n",
      "Epoch 213 | Train loss: 2.3972 | Val loss: 2.3418\n",
      "Epoch 214 | Train loss: 2.3735 | Val loss: 2.3611\n",
      "Epoch 215 | Train loss: 2.3954 | Val loss: 2.3423\n",
      "Epoch 216 | Train loss: 2.3756 | Val loss: 2.3375\n",
      "Epoch 217 | Train loss: 2.3714 | Val loss: 2.3645\n",
      "Epoch 218 | Train loss: 2.3847 | Val loss: 2.3438\n",
      "Epoch 219 | Train loss: 2.3999 | Val loss: 2.3465\n",
      "Epoch 220 | Train loss: 2.3699 | Val loss: 2.3652\n",
      "Epoch 221 | Train loss: 2.3801 | Val loss: 2.3627\n",
      "Epoch 222 | Train loss: 2.3899 | Val loss: 2.3638\n",
      "Epoch 223 | Train loss: 2.3845 | Val loss: 2.3531\n",
      "Epoch 224 | Train loss: 2.3785 | Val loss: 2.3425\n",
      "Epoch 225 | Train loss: 2.3712 | Val loss: 2.3419\n",
      "Epoch 226 | Train loss: 2.3696 | Val loss: 2.3358\n",
      "Epoch 227 | Train loss: 2.3540 | Val loss: 2.3390\n",
      "Epoch 228 | Train loss: 2.3596 | Val loss: 2.3558\n",
      "Epoch 229 | Train loss: 2.3767 | Val loss: 2.3461\n",
      "Epoch 230 | Train loss: 2.3806 | Val loss: 2.3207\n",
      "Epoch 231 | Train loss: 2.3659 | Val loss: 2.3262\n",
      "Epoch 232 | Train loss: 2.3840 | Val loss: 2.3253\n",
      "Epoch 233 | Train loss: 2.3698 | Val loss: 2.3155\n",
      "Epoch 234 | Train loss: 2.3687 | Val loss: 2.3084\n",
      "Epoch 235 | Train loss: 2.3546 | Val loss: 2.3298\n",
      "Epoch 236 | Train loss: 2.3417 | Val loss: 2.2965\n",
      "Epoch 237 | Train loss: 2.3551 | Val loss: 2.3371\n",
      "Epoch 238 | Train loss: 2.3526 | Val loss: 2.3180\n",
      "Epoch 239 | Train loss: 2.3514 | Val loss: 2.2993\n",
      "Epoch 240 | Train loss: 2.3570 | Val loss: 2.3272\n",
      "Epoch 241 | Train loss: 2.3502 | Val loss: 2.2963\n",
      "Epoch 242 | Train loss: 2.3666 | Val loss: 2.3094\n",
      "Epoch 243 | Train loss: 2.3473 | Val loss: 2.2790\n",
      "Epoch 244 | Train loss: 2.3579 | Val loss: 2.3094\n",
      "Epoch 245 | Train loss: 2.3452 | Val loss: 2.3178\n",
      "Epoch 246 | Train loss: 2.3431 | Val loss: 2.3021\n",
      "Epoch 247 | Train loss: 2.3410 | Val loss: 2.2974\n",
      "Epoch 248 | Train loss: 2.3513 | Val loss: 2.2920\n",
      "Epoch 249 | Train loss: 2.3389 | Val loss: 2.2946\n",
      "Epoch 250 | Train loss: 2.3467 | Val loss: 2.3273\n",
      "Epoch 251 | Train loss: 2.3369 | Val loss: 2.2845\n",
      "Epoch 252 | Train loss: 2.3253 | Val loss: 2.3174\n",
      "Epoch 253 | Train loss: 2.3139 | Val loss: 2.2804\n",
      "Epoch 254 | Train loss: 2.3322 | Val loss: 2.2926\n",
      "Epoch 255 | Train loss: 2.3321 | Val loss: 2.3230\n",
      "Epoch 256 | Train loss: 2.3392 | Val loss: 2.2821\n",
      "Epoch 257 | Train loss: 2.3133 | Val loss: 2.2813\n",
      "Epoch 258 | Train loss: 2.3230 | Val loss: 2.3004\n",
      "Epoch 259 | Train loss: 2.3417 | Val loss: 2.2851\n",
      "Epoch 260 | Train loss: 2.3172 | Val loss: 2.2685\n",
      "Epoch 261 | Train loss: 2.3373 | Val loss: 2.2909\n",
      "Epoch 262 | Train loss: 2.3266 | Val loss: 2.2780\n",
      "Epoch 263 | Train loss: 2.3247 | Val loss: 2.2689\n",
      "Epoch 264 | Train loss: 2.3163 | Val loss: 2.2559\n",
      "Epoch 265 | Train loss: 2.3302 | Val loss: 2.2759\n",
      "Epoch 266 | Train loss: 2.3202 | Val loss: 2.2550\n",
      "Epoch 267 | Train loss: 2.3168 | Val loss: 2.3008\n",
      "Epoch 268 | Train loss: 2.3199 | Val loss: 2.2844\n",
      "Epoch 269 | Train loss: 2.3170 | Val loss: 2.2578\n",
      "Epoch 270 | Train loss: 2.3023 | Val loss: 2.2902\n",
      "Epoch 271 | Train loss: 2.3253 | Val loss: 2.2596\n",
      "Epoch 272 | Train loss: 2.3048 | Val loss: 2.2662\n",
      "Epoch 273 | Train loss: 2.3092 | Val loss: 2.2412\n",
      "Epoch 274 | Train loss: 2.3025 | Val loss: 2.2542\n",
      "Epoch 275 | Train loss: 2.2972 | Val loss: 2.2615\n",
      "Epoch 276 | Train loss: 2.3221 | Val loss: 2.2706\n",
      "Epoch 277 | Train loss: 2.2711 | Val loss: 2.2591\n",
      "Epoch 278 | Train loss: 2.3055 | Val loss: 2.2643\n",
      "Epoch 279 | Train loss: 2.3203 | Val loss: 2.2766\n",
      "Epoch 280 | Train loss: 2.3031 | Val loss: 2.2609\n",
      "Epoch 281 | Train loss: 2.2731 | Val loss: 2.2692\n",
      "Epoch 282 | Train loss: 2.2970 | Val loss: 2.2744\n",
      "Epoch 283 | Train loss: 2.3062 | Val loss: 2.2485\n",
      "Epoch 284 | Train loss: 2.2858 | Val loss: 2.2647\n",
      "Epoch 285 | Train loss: 2.2866 | Val loss: 2.2731\n",
      "Epoch 286 | Train loss: 2.3151 | Val loss: 2.2645\n",
      "Epoch 287 | Train loss: 2.2886 | Val loss: 2.2470\n",
      "Epoch 288 | Train loss: 2.2756 | Val loss: 2.2423\n",
      "Epoch 289 | Train loss: 2.2950 | Val loss: 2.2469\n",
      "Epoch 290 | Train loss: 2.2763 | Val loss: 2.2584\n",
      "Epoch 291 | Train loss: 2.2891 | Val loss: 2.2403\n",
      "Epoch 292 | Train loss: 2.2816 | Val loss: 2.2344\n",
      "Epoch 293 | Train loss: 2.2819 | Val loss: 2.2257\n",
      "Epoch 294 | Train loss: 2.2845 | Val loss: 2.2286\n",
      "Epoch 295 | Train loss: 2.2697 | Val loss: 2.2605\n",
      "Epoch 296 | Train loss: 2.2751 | Val loss: 2.2360\n",
      "Epoch 297 | Train loss: 2.2763 | Val loss: 2.2551\n",
      "Epoch 298 | Train loss: 2.2895 | Val loss: 2.2366\n",
      "Epoch 299 | Train loss: 2.2701 | Val loss: 2.2425\n",
      "Epoch 300 | Train loss: 2.2995 | Val loss: 2.2291\n",
      "Epoch 301 | Train loss: 2.2786 | Val loss: 2.2372\n",
      "Epoch 302 | Train loss: 2.2524 | Val loss: 2.2229\n",
      "Epoch 303 | Train loss: 2.2589 | Val loss: 2.2180\n",
      "Epoch 304 | Train loss: 2.2707 | Val loss: 2.2438\n",
      "Epoch 305 | Train loss: 2.2541 | Val loss: 2.2335\n",
      "Epoch 306 | Train loss: 2.2344 | Val loss: 2.2241\n",
      "Epoch 307 | Train loss: 2.2643 | Val loss: 2.2286\n",
      "Epoch 308 | Train loss: 2.2421 | Val loss: 2.2315\n",
      "Epoch 309 | Train loss: 2.2816 | Val loss: 2.2152\n",
      "Epoch 310 | Train loss: 2.2567 | Val loss: 2.2368\n",
      "Epoch 311 | Train loss: 2.2414 | Val loss: 2.2237\n",
      "Epoch 312 | Train loss: 2.2377 | Val loss: 2.2281\n",
      "Epoch 313 | Train loss: 2.2617 | Val loss: 2.2046\n",
      "Epoch 314 | Train loss: 2.2518 | Val loss: 2.2353\n",
      "Epoch 315 | Train loss: 2.2472 | Val loss: 2.2069\n",
      "Epoch 316 | Train loss: 2.2683 | Val loss: 2.2004\n",
      "Epoch 317 | Train loss: 2.2569 | Val loss: 2.2176\n",
      "Epoch 318 | Train loss: 2.2458 | Val loss: 2.2173\n",
      "Epoch 319 | Train loss: 2.2189 | Val loss: 2.2024\n",
      "Epoch 320 | Train loss: 2.2455 | Val loss: 2.1940\n",
      "Epoch 321 | Train loss: 2.2366 | Val loss: 2.2208\n",
      "Epoch 322 | Train loss: 2.2763 | Val loss: 2.1965\n",
      "Epoch 323 | Train loss: 2.2495 | Val loss: 2.1876\n",
      "Epoch 324 | Train loss: 2.2375 | Val loss: 2.2059\n",
      "Epoch 325 | Train loss: 2.2217 | Val loss: 2.1876\n",
      "Epoch 326 | Train loss: 2.2317 | Val loss: 2.1961\n",
      "Epoch 327 | Train loss: 2.2479 | Val loss: 2.2007\n",
      "Epoch 328 | Train loss: 2.2254 | Val loss: 2.2128\n",
      "Epoch 329 | Train loss: 2.2502 | Val loss: 2.1726\n",
      "Epoch 330 | Train loss: 2.2339 | Val loss: 2.1854\n",
      "Epoch 331 | Train loss: 2.2078 | Val loss: 2.2038\n",
      "Epoch 332 | Train loss: 2.2510 | Val loss: 2.1835\n",
      "Epoch 333 | Train loss: 2.2428 | Val loss: 2.1942\n",
      "Epoch 334 | Train loss: 2.2230 | Val loss: 2.1839\n",
      "Epoch 335 | Train loss: 2.2441 | Val loss: 2.1743\n",
      "Epoch 336 | Train loss: 2.2290 | Val loss: 2.1861\n",
      "Epoch 337 | Train loss: 2.2435 | Val loss: 2.1857\n",
      "Epoch 338 | Train loss: 2.2239 | Val loss: 2.2113\n",
      "Epoch 339 | Train loss: 2.2178 | Val loss: 2.1671\n",
      "Epoch 340 | Train loss: 2.2091 | Val loss: 2.1583\n",
      "Epoch 341 | Train loss: 2.2246 | Val loss: 2.1486\n",
      "Epoch 342 | Train loss: 2.2345 | Val loss: 2.1699\n",
      "Epoch 343 | Train loss: 2.1896 | Val loss: 2.1912\n",
      "Epoch 344 | Train loss: 2.2072 | Val loss: 2.1749\n",
      "Epoch 345 | Train loss: 2.1816 | Val loss: 2.1780\n",
      "Epoch 346 | Train loss: 2.2146 | Val loss: 2.1919\n",
      "Epoch 347 | Train loss: 2.2227 | Val loss: 2.1550\n",
      "Epoch 348 | Train loss: 2.2007 | Val loss: 2.1853\n",
      "Epoch 349 | Train loss: 2.1949 | Val loss: 2.1853\n",
      "Epoch 350 | Train loss: 2.2285 | Val loss: 2.1628\n",
      "Epoch 351 | Train loss: 2.2077 | Val loss: 2.1428\n",
      "Epoch 352 | Train loss: 2.1940 | Val loss: 2.1667\n",
      "Epoch 353 | Train loss: 2.1816 | Val loss: 2.1736\n",
      "Epoch 354 | Train loss: 2.2017 | Val loss: 2.1828\n",
      "Epoch 355 | Train loss: 2.1980 | Val loss: 2.1440\n",
      "Epoch 356 | Train loss: 2.1853 | Val loss: 2.1673\n",
      "Epoch 357 | Train loss: 2.1919 | Val loss: 2.1665\n",
      "Epoch 358 | Train loss: 2.1913 | Val loss: 2.1489\n",
      "Epoch 359 | Train loss: 2.1930 | Val loss: 2.1475\n",
      "Epoch 360 | Train loss: 2.2106 | Val loss: 2.1571\n",
      "Epoch 361 | Train loss: 2.1746 | Val loss: 2.1712\n",
      "Epoch 362 | Train loss: 2.2072 | Val loss: 2.1528\n",
      "Epoch 363 | Train loss: 2.2026 | Val loss: 2.1629\n",
      "Epoch 364 | Train loss: 2.1707 | Val loss: 2.1536\n",
      "Epoch 365 | Train loss: 2.1956 | Val loss: 2.1444\n",
      "Epoch 366 | Train loss: 2.1974 | Val loss: 2.2036\n",
      "Epoch 367 | Train loss: 2.1868 | Val loss: 2.1360\n",
      "Epoch 368 | Train loss: 2.1800 | Val loss: 2.1568\n",
      "Epoch 369 | Train loss: 2.1861 | Val loss: 2.1600\n",
      "Epoch 370 | Train loss: 2.1671 | Val loss: 2.1431\n",
      "Epoch 371 | Train loss: 2.1895 | Val loss: 2.1582\n",
      "Epoch 372 | Train loss: 2.1989 | Val loss: 2.1592\n",
      "Epoch 373 | Train loss: 2.1806 | Val loss: 2.1602\n",
      "Epoch 374 | Train loss: 2.1557 | Val loss: 2.1619\n",
      "Epoch 375 | Train loss: 2.1744 | Val loss: 2.1363\n",
      "Epoch 376 | Train loss: 2.1954 | Val loss: 2.1269\n",
      "Epoch 377 | Train loss: 2.1737 | Val loss: 2.1456\n",
      "Epoch 378 | Train loss: 2.1901 | Val loss: 2.1576\n",
      "Epoch 379 | Train loss: 2.2008 | Val loss: 2.1443\n",
      "Epoch 380 | Train loss: 2.1715 | Val loss: 2.1462\n",
      "Epoch 381 | Train loss: 2.1692 | Val loss: 2.1303\n",
      "Epoch 382 | Train loss: 2.1700 | Val loss: 2.1541\n",
      "Epoch 383 | Train loss: 2.1457 | Val loss: 2.1194\n",
      "Epoch 384 | Train loss: 2.1687 | Val loss: 2.1548\n",
      "Epoch 385 | Train loss: 2.1674 | Val loss: 2.1422\n",
      "Epoch 386 | Train loss: 2.1404 | Val loss: 2.1557\n",
      "Epoch 387 | Train loss: 2.1752 | Val loss: 2.1441\n",
      "Epoch 388 | Train loss: 2.1543 | Val loss: 2.1374\n",
      "Epoch 389 | Train loss: 2.1716 | Val loss: 2.1199\n",
      "Epoch 390 | Train loss: 2.1768 | Val loss: 2.1542\n",
      "Epoch 391 | Train loss: 2.1588 | Val loss: 2.1411\n",
      "Epoch 392 | Train loss: 2.1578 | Val loss: 2.1331\n",
      "Epoch 393 | Train loss: 2.1609 | Val loss: 2.1093\n",
      "Epoch 394 | Train loss: 2.1895 | Val loss: 2.1174\n",
      "Epoch 395 | Train loss: 2.1576 | Val loss: 2.1258\n",
      "Epoch 396 | Train loss: 2.1423 | Val loss: 2.1134\n",
      "Epoch 397 | Train loss: 2.1432 | Val loss: 2.1135\n",
      "Epoch 398 | Train loss: 2.1658 | Val loss: 2.1448\n",
      "Epoch 399 | Train loss: 2.1678 | Val loss: 2.1102\n",
      "Epoch 400 | Train loss: 2.1671 | Val loss: 2.1172\n",
      "Epoch 401 | Train loss: 2.1321 | Val loss: 2.1058\n",
      "Epoch 402 | Train loss: 2.1301 | Val loss: 2.0904\n",
      "Epoch 403 | Train loss: 2.1104 | Val loss: 2.1055\n",
      "Epoch 404 | Train loss: 2.1413 | Val loss: 2.1472\n",
      "Epoch 405 | Train loss: 2.1468 | Val loss: 2.1327\n",
      "Epoch 406 | Train loss: 2.1374 | Val loss: 2.1297\n",
      "Epoch 407 | Train loss: 2.1473 | Val loss: 2.1641\n",
      "Epoch 408 | Train loss: 2.1344 | Val loss: 2.0928\n",
      "Epoch 409 | Train loss: 2.1403 | Val loss: 2.1157\n",
      "Epoch 410 | Train loss: 2.1170 | Val loss: 2.0862\n",
      "Epoch 411 | Train loss: 2.1644 | Val loss: 2.1143\n",
      "Epoch 412 | Train loss: 2.1308 | Val loss: 2.1121\n",
      "Epoch 413 | Train loss: 2.1517 | Val loss: 2.1340\n",
      "Epoch 414 | Train loss: 2.1173 | Val loss: 2.1238\n",
      "Epoch 415 | Train loss: 2.1434 | Val loss: 2.1242\n",
      "Epoch 416 | Train loss: 2.1276 | Val loss: 2.0916\n",
      "Epoch 417 | Train loss: 2.1391 | Val loss: 2.1480\n",
      "Epoch 418 | Train loss: 2.1142 | Val loss: 2.1108\n",
      "Epoch 419 | Train loss: 2.1319 | Val loss: 2.1070\n",
      "Epoch 420 | Train loss: 2.1314 | Val loss: 2.1038\n",
      "Epoch 421 | Train loss: 2.1316 | Val loss: 2.0902\n",
      "Epoch 422 | Train loss: 2.1213 | Val loss: 2.1265\n",
      "Epoch 423 | Train loss: 2.1109 | Val loss: 2.1095\n",
      "Epoch 424 | Train loss: 2.1213 | Val loss: 2.0916\n",
      "Epoch 425 | Train loss: 2.1139 | Val loss: 2.0923\n",
      "Epoch 426 | Train loss: 2.1149 | Val loss: 2.0994\n",
      "Epoch 427 | Train loss: 2.1047 | Val loss: 2.0819\n",
      "Epoch 428 | Train loss: 2.1078 | Val loss: 2.1219\n",
      "Epoch 429 | Train loss: 2.0924 | Val loss: 2.1247\n",
      "Epoch 430 | Train loss: 2.1284 | Val loss: 2.1022\n",
      "Epoch 431 | Train loss: 2.1210 | Val loss: 2.1025\n",
      "Epoch 432 | Train loss: 2.1270 | Val loss: 2.0756\n",
      "Epoch 433 | Train loss: 2.0924 | Val loss: 2.1086\n",
      "Epoch 434 | Train loss: 2.0991 | Val loss: 2.0676\n",
      "Epoch 435 | Train loss: 2.1016 | Val loss: 2.0725\n",
      "Epoch 436 | Train loss: 2.1069 | Val loss: 2.0801\n",
      "Epoch 437 | Train loss: 2.1111 | Val loss: 2.1109\n",
      "Epoch 438 | Train loss: 2.0993 | Val loss: 2.1023\n",
      "Epoch 439 | Train loss: 2.1151 | Val loss: 2.0700\n",
      "Epoch 440 | Train loss: 2.0871 | Val loss: 2.0809\n",
      "Epoch 441 | Train loss: 2.1195 | Val loss: 2.0817\n",
      "Epoch 442 | Train loss: 2.0965 | Val loss: 2.0781\n",
      "Epoch 443 | Train loss: 2.1104 | Val loss: 2.0925\n",
      "Epoch 444 | Train loss: 2.0870 | Val loss: 2.0638\n",
      "Epoch 445 | Train loss: 2.1233 | Val loss: 2.0729\n",
      "Epoch 446 | Train loss: 2.0733 | Val loss: 2.1037\n",
      "Epoch 447 | Train loss: 2.0926 | Val loss: 2.0691\n",
      "Epoch 448 | Train loss: 2.1016 | Val loss: 2.1016\n",
      "Epoch 449 | Train loss: 2.1145 | Val loss: 2.0923\n",
      "Epoch 450 | Train loss: 2.1183 | Val loss: 2.0425\n",
      "Epoch 451 | Train loss: 2.0903 | Val loss: 2.0714\n",
      "Epoch 452 | Train loss: 2.0882 | Val loss: 2.0670\n",
      "Epoch 453 | Train loss: 2.0767 | Val loss: 2.1051\n",
      "Epoch 454 | Train loss: 2.0836 | Val loss: 2.0551\n",
      "Epoch 455 | Train loss: 2.0998 | Val loss: 2.0539\n",
      "Epoch 456 | Train loss: 2.0939 | Val loss: 2.0729\n",
      "Epoch 457 | Train loss: 2.0978 | Val loss: 2.0703\n",
      "Epoch 458 | Train loss: 2.1049 | Val loss: 2.0658\n",
      "Epoch 459 | Train loss: 2.0667 | Val loss: 2.0606\n",
      "Epoch 460 | Train loss: 2.0601 | Val loss: 2.0710\n",
      "Epoch 461 | Train loss: 2.1093 | Val loss: 2.0927\n",
      "Epoch 462 | Train loss: 2.0715 | Val loss: 2.0597\n",
      "Epoch 463 | Train loss: 2.0625 | Val loss: 2.0382\n",
      "Epoch 464 | Train loss: 2.0751 | Val loss: 2.0685\n",
      "Epoch 465 | Train loss: 2.0585 | Val loss: 2.0503\n",
      "Epoch 466 | Train loss: 2.0702 | Val loss: 2.0529\n",
      "Epoch 467 | Train loss: 2.0631 | Val loss: 2.1010\n",
      "Epoch 468 | Train loss: 2.0937 | Val loss: 2.0470\n",
      "Epoch 469 | Train loss: 2.0874 | Val loss: 2.0363\n",
      "Epoch 470 | Train loss: 2.0911 | Val loss: 2.0576\n",
      "Epoch 471 | Train loss: 2.0707 | Val loss: 2.0444\n",
      "Epoch 472 | Train loss: 2.0702 | Val loss: 2.0624\n",
      "Epoch 473 | Train loss: 2.0657 | Val loss: 2.0627\n",
      "Epoch 474 | Train loss: 2.0729 | Val loss: 2.0569\n",
      "Epoch 475 | Train loss: 2.0572 | Val loss: 2.0589\n",
      "Epoch 476 | Train loss: 2.0614 | Val loss: 2.0584\n",
      "Epoch 477 | Train loss: 2.0595 | Val loss: 2.0469\n",
      "Epoch 478 | Train loss: 2.0464 | Val loss: 2.0690\n",
      "Epoch 479 | Train loss: 2.0427 | Val loss: 2.0328\n",
      "Epoch 480 | Train loss: 2.0663 | Val loss: 2.0339\n",
      "Epoch 481 | Train loss: 2.0384 | Val loss: 2.0514\n",
      "Epoch 482 | Train loss: 2.0568 | Val loss: 2.0316\n",
      "Epoch 483 | Train loss: 2.0859 | Val loss: 2.0316\n",
      "Epoch 484 | Train loss: 2.0216 | Val loss: 2.0583\n",
      "Epoch 485 | Train loss: 2.0604 | Val loss: 2.0557\n",
      "Epoch 486 | Train loss: 2.0857 | Val loss: 2.0595\n",
      "Epoch 487 | Train loss: 2.0351 | Val loss: 2.0651\n",
      "Epoch 488 | Train loss: 2.0673 | Val loss: 2.0582\n",
      "Epoch 489 | Train loss: 2.0155 | Val loss: 2.0082\n",
      "Epoch 490 | Train loss: 2.0888 | Val loss: 2.0599\n",
      "Epoch 491 | Train loss: 2.0819 | Val loss: 2.0380\n",
      "Epoch 492 | Train loss: 2.0592 | Val loss: 2.0701\n",
      "Epoch 493 | Train loss: 2.0457 | Val loss: 2.0201\n",
      "Epoch 494 | Train loss: 2.0394 | Val loss: 2.0235\n",
      "Epoch 495 | Train loss: 2.0521 | Val loss: 1.9885\n",
      "Epoch 496 | Train loss: 2.0267 | Val loss: 2.0133\n",
      "Epoch 497 | Train loss: 2.0467 | Val loss: 2.0456\n",
      "Epoch 498 | Train loss: 2.0265 | Val loss: 2.0203\n",
      "Epoch 499 | Train loss: 2.0329 | Val loss: 2.0333\n",
      "Epoch 500 | Train loss: 2.0531 | Val loss: 2.0231\n",
      "Epoch 501 | Train loss: 2.0192 | Val loss: 1.9953\n",
      "Epoch 502 | Train loss: 2.0501 | Val loss: 2.0240\n",
      "Epoch 503 | Train loss: 2.0296 | Val loss: 2.0325\n",
      "Epoch 504 | Train loss: 2.0319 | Val loss: 2.0130\n",
      "Epoch 505 | Train loss: 2.0067 | Val loss: 2.0320\n",
      "Epoch 506 | Train loss: 1.9948 | Val loss: 2.0393\n",
      "Epoch 507 | Train loss: 2.0485 | Val loss: 2.0081\n",
      "Epoch 508 | Train loss: 2.0172 | Val loss: 2.0075\n",
      "Epoch 509 | Train loss: 2.0173 | Val loss: 2.0124\n",
      "Epoch 510 | Train loss: 2.0325 | Val loss: 2.0498\n",
      "Epoch 511 | Train loss: 2.0314 | Val loss: 2.0167\n",
      "Epoch 512 | Train loss: 2.0382 | Val loss: 2.0208\n",
      "Epoch 513 | Train loss: 1.9869 | Val loss: 2.0430\n",
      "Epoch 514 | Train loss: 2.0081 | Val loss: 1.9965\n",
      "Epoch 515 | Train loss: 2.0358 | Val loss: 1.9791\n",
      "Epoch 516 | Train loss: 2.0178 | Val loss: 2.0385\n",
      "Epoch 517 | Train loss: 1.9657 | Val loss: 2.0306\n",
      "Epoch 518 | Train loss: 2.0010 | Val loss: 2.0438\n",
      "Epoch 519 | Train loss: 2.0215 | Val loss: 2.0347\n",
      "Epoch 520 | Train loss: 2.0317 | Val loss: 2.0057\n",
      "Epoch 521 | Train loss: 2.0331 | Val loss: 2.0376\n",
      "Epoch 522 | Train loss: 2.0156 | Val loss: 2.0227\n",
      "Epoch 523 | Train loss: 2.0685 | Val loss: 1.9915\n",
      "Epoch 524 | Train loss: 1.9992 | Val loss: 2.0475\n",
      "Epoch 525 | Train loss: 2.0228 | Val loss: 1.9973\n",
      "Epoch 526 | Train loss: 2.0193 | Val loss: 1.9953\n",
      "Epoch 527 | Train loss: 2.0026 | Val loss: 2.0069\n",
      "Epoch 528 | Train loss: 2.0199 | Val loss: 2.0198\n",
      "Epoch 529 | Train loss: 2.0138 | Val loss: 2.0086\n",
      "Epoch 530 | Train loss: 2.0333 | Val loss: 1.9844\n",
      "Epoch 531 | Train loss: 2.0141 | Val loss: 1.9682\n",
      "Epoch 532 | Train loss: 1.9625 | Val loss: 2.0149\n",
      "Epoch 533 | Train loss: 2.0121 | Val loss: 2.0175\n",
      "Epoch 534 | Train loss: 2.0102 | Val loss: 2.0192\n",
      "Epoch 535 | Train loss: 2.0272 | Val loss: 1.9844\n",
      "Epoch 536 | Train loss: 1.9820 | Val loss: 1.9812\n",
      "Epoch 537 | Train loss: 2.0243 | Val loss: 2.0015\n",
      "Epoch 538 | Train loss: 1.9863 | Val loss: 1.9843\n",
      "Epoch 539 | Train loss: 1.9415 | Val loss: 2.0373\n",
      "Epoch 540 | Train loss: 1.9860 | Val loss: 2.0097\n",
      "Epoch 541 | Train loss: 1.9734 | Val loss: 2.0388\n",
      "Epoch 542 | Train loss: 2.0109 | Val loss: 1.9871\n",
      "Epoch 543 | Train loss: 1.9887 | Val loss: 1.9790\n",
      "Epoch 544 | Train loss: 1.9932 | Val loss: 2.0194\n",
      "Epoch 545 | Train loss: 1.9711 | Val loss: 1.9991\n",
      "Epoch 546 | Train loss: 1.9989 | Val loss: 1.9763\n",
      "Epoch 547 | Train loss: 2.0271 | Val loss: 1.9752\n",
      "Epoch 548 | Train loss: 1.9674 | Val loss: 2.0310\n",
      "Epoch 549 | Train loss: 1.9942 | Val loss: 2.0061\n",
      "Epoch 550 | Train loss: 1.9810 | Val loss: 1.9989\n",
      "Epoch 551 | Train loss: 1.9822 | Val loss: 1.9988\n",
      "Epoch 552 | Train loss: 2.0085 | Val loss: 2.0089\n",
      "Epoch 553 | Train loss: 1.9805 | Val loss: 2.0049\n",
      "Epoch 554 | Train loss: 1.9584 | Val loss: 1.9802\n",
      "Epoch 555 | Train loss: 1.9639 | Val loss: 2.0126\n",
      "Epoch 556 | Train loss: 1.9813 | Val loss: 1.9905\n",
      "Epoch 557 | Train loss: 1.9732 | Val loss: 1.9744\n",
      "Epoch 558 | Train loss: 2.0016 | Val loss: 1.9845\n",
      "Epoch 559 | Train loss: 2.0127 | Val loss: 1.9668\n",
      "Epoch 560 | Train loss: 1.9508 | Val loss: 2.0053\n",
      "Epoch 561 | Train loss: 1.9648 | Val loss: 2.0086\n",
      "Epoch 562 | Train loss: 1.9784 | Val loss: 1.9705\n",
      "Epoch 563 | Train loss: 1.9833 | Val loss: 2.0028\n",
      "Epoch 564 | Train loss: 1.9839 | Val loss: 1.9745\n",
      "Epoch 565 | Train loss: 1.9874 | Val loss: 1.9826\n",
      "Epoch 566 | Train loss: 1.9983 | Val loss: 1.9989\n",
      "Epoch 567 | Train loss: 1.9644 | Val loss: 1.9545\n",
      "Epoch 568 | Train loss: 1.9957 | Val loss: 1.9796\n",
      "Epoch 569 | Train loss: 1.9878 | Val loss: 1.9498\n",
      "Epoch 570 | Train loss: 1.9607 | Val loss: 1.9621\n",
      "Epoch 571 | Train loss: 1.9763 | Val loss: 1.9682\n",
      "Epoch 572 | Train loss: 1.9613 | Val loss: 1.9649\n",
      "Epoch 573 | Train loss: 1.9701 | Val loss: 1.9702\n",
      "Epoch 574 | Train loss: 1.9602 | Val loss: 1.9836\n",
      "Epoch 575 | Train loss: 1.9619 | Val loss: 1.9598\n",
      "Epoch 576 | Train loss: 1.9915 | Val loss: 1.9646\n",
      "Epoch 577 | Train loss: 1.9850 | Val loss: 1.9867\n",
      "Epoch 578 | Train loss: 1.9614 | Val loss: 1.9944\n",
      "Epoch 579 | Train loss: 1.9734 | Val loss: 1.9774\n",
      "Epoch 580 | Train loss: 1.9529 | Val loss: 1.9958\n",
      "Epoch 581 | Train loss: 1.9709 | Val loss: 1.9944\n",
      "Epoch 582 | Train loss: 1.9715 | Val loss: 1.9903\n",
      "Epoch 583 | Train loss: 1.9720 | Val loss: 2.0089\n",
      "Epoch 584 | Train loss: 1.9729 | Val loss: 1.9507\n",
      "Epoch 585 | Train loss: 1.9329 | Val loss: 1.9407\n",
      "Epoch 586 | Train loss: 1.9486 | Val loss: 1.9622\n",
      "Epoch 587 | Train loss: 1.9512 | Val loss: 2.0026\n",
      "Epoch 588 | Train loss: 1.9466 | Val loss: 1.9715\n",
      "Epoch 589 | Train loss: 1.9362 | Val loss: 1.9546\n",
      "Epoch 590 | Train loss: 1.9108 | Val loss: 1.9547\n",
      "Epoch 591 | Train loss: 1.9417 | Val loss: 2.0031\n",
      "Epoch 592 | Train loss: 1.9535 | Val loss: 1.9815\n",
      "Epoch 593 | Train loss: 1.9189 | Val loss: 1.9472\n",
      "Epoch 594 | Train loss: 1.9252 | Val loss: 1.9922\n",
      "Epoch 595 | Train loss: 1.9223 | Val loss: 1.9703\n",
      "Epoch 596 | Train loss: 1.9365 | Val loss: 1.9611\n",
      "Epoch 597 | Train loss: 1.9348 | Val loss: 1.9372\n",
      "Epoch 598 | Train loss: 1.9276 | Val loss: 1.9909\n",
      "Epoch 599 | Train loss: 1.9343 | Val loss: 1.9646\n",
      "Epoch 600 | Train loss: 1.9521 | Val loss: 1.9818\n",
      "Epoch 601 | Train loss: 1.9230 | Val loss: 1.9406\n",
      "Epoch 602 | Train loss: 1.9186 | Val loss: 1.9521\n",
      "Epoch 603 | Train loss: 1.9511 | Val loss: 1.9402\n",
      "Epoch 604 | Train loss: 1.9401 | Val loss: 1.9823\n",
      "Epoch 605 | Train loss: 1.9715 | Val loss: 1.9758\n",
      "Epoch 606 | Train loss: 1.9494 | Val loss: 1.9341\n",
      "Epoch 607 | Train loss: 1.9282 | Val loss: 1.9448\n",
      "Epoch 608 | Train loss: 1.9850 | Val loss: 1.9443\n",
      "Epoch 609 | Train loss: 1.9169 | Val loss: 1.9703\n",
      "Epoch 610 | Train loss: 1.9443 | Val loss: 1.9608\n",
      "Epoch 611 | Train loss: 1.9288 | Val loss: 1.9622\n",
      "Epoch 612 | Train loss: 1.9488 | Val loss: 1.9780\n",
      "Epoch 613 | Train loss: 1.9373 | Val loss: 1.9604\n",
      "Epoch 614 | Train loss: 1.9527 | Val loss: 1.9457\n",
      "Epoch 615 | Train loss: 1.9415 | Val loss: 1.9741\n",
      "Epoch 616 | Train loss: 1.9233 | Val loss: 1.9542\n",
      "Epoch 617 | Train loss: 1.9392 | Val loss: 1.9407\n",
      "Epoch 618 | Train loss: 1.9493 | Val loss: 1.9467\n",
      "Epoch 619 | Train loss: 1.9357 | Val loss: 1.9721\n",
      "Epoch 620 | Train loss: 1.9065 | Val loss: 1.9518\n",
      "Epoch 621 | Train loss: 1.9084 | Val loss: 1.9438\n",
      "Epoch 622 | Train loss: 1.9176 | Val loss: 1.9363\n",
      "Epoch 623 | Train loss: 1.9112 | Val loss: 1.9488\n",
      "Epoch 624 | Train loss: 1.9347 | Val loss: 1.9548\n",
      "Epoch 625 | Train loss: 1.9045 | Val loss: 1.9466\n",
      "Epoch 626 | Train loss: 1.9381 | Val loss: 1.9370\n",
      "Epoch 627 | Train loss: 1.9341 | Val loss: 1.9205\n",
      "Epoch 628 | Train loss: 1.9242 | Val loss: 1.9481\n",
      "Epoch 629 | Train loss: 1.9374 | Val loss: 1.9709\n",
      "Epoch 630 | Train loss: 1.9310 | Val loss: 1.9706\n",
      "Epoch 631 | Train loss: 1.9080 | Val loss: 1.9465\n",
      "Epoch 632 | Train loss: 1.9403 | Val loss: 1.9035\n",
      "Epoch 633 | Train loss: 1.9503 | Val loss: 1.9357\n",
      "Epoch 634 | Train loss: 1.8952 | Val loss: 1.9455\n",
      "Epoch 635 | Train loss: 1.9370 | Val loss: 1.9486\n",
      "Epoch 636 | Train loss: 1.8923 | Val loss: 1.8708\n",
      "Epoch 637 | Train loss: 1.9129 | Val loss: 1.9175\n",
      "Epoch 638 | Train loss: 1.8958 | Val loss: 1.8929\n",
      "Epoch 639 | Train loss: 1.9200 | Val loss: 1.9235\n",
      "Epoch 640 | Train loss: 1.9048 | Val loss: 1.9300\n",
      "Epoch 641 | Train loss: 1.9078 | Val loss: 1.9251\n",
      "Epoch 642 | Train loss: 1.9188 | Val loss: 1.9383\n",
      "Epoch 643 | Train loss: 1.9059 | Val loss: 1.9532\n",
      "Epoch 644 | Train loss: 1.9029 | Val loss: 1.9276\n",
      "Epoch 645 | Train loss: 1.9050 | Val loss: 1.9397\n",
      "Epoch 646 | Train loss: 1.9225 | Val loss: 1.8713\n",
      "Epoch 647 | Train loss: 1.9004 | Val loss: 1.9146\n",
      "Epoch 648 | Train loss: 1.8944 | Val loss: 1.9188\n",
      "Epoch 649 | Train loss: 1.8862 | Val loss: 1.9103\n",
      "Epoch 650 | Train loss: 1.9033 | Val loss: 1.8917\n",
      "Epoch 651 | Train loss: 1.8812 | Val loss: 1.9618\n",
      "Epoch 652 | Train loss: 1.9022 | Val loss: 1.8982\n",
      "Epoch 653 | Train loss: 1.8806 | Val loss: 1.9173\n",
      "Epoch 654 | Train loss: 1.8619 | Val loss: 1.9076\n",
      "Epoch 655 | Train loss: 1.8918 | Val loss: 1.8980\n",
      "Epoch 656 | Train loss: 1.8951 | Val loss: 1.8504\n",
      "Epoch 657 | Train loss: 1.9035 | Val loss: 1.9411\n",
      "Epoch 658 | Train loss: 1.9002 | Val loss: 1.9282\n",
      "Epoch 659 | Train loss: 1.8776 | Val loss: 1.9186\n",
      "Epoch 660 | Train loss: 1.8990 | Val loss: 1.9518\n",
      "Epoch 661 | Train loss: 1.8825 | Val loss: 1.9252\n",
      "Epoch 662 | Train loss: 1.9028 | Val loss: 1.9330\n",
      "Epoch 663 | Train loss: 1.8769 | Val loss: 1.8970\n",
      "Epoch 664 | Train loss: 1.8769 | Val loss: 1.9332\n",
      "Epoch 665 | Train loss: 1.8787 | Val loss: 1.8900\n",
      "Epoch 666 | Train loss: 1.8873 | Val loss: 1.9274\n",
      "Epoch 667 | Train loss: 1.8831 | Val loss: 1.9421\n",
      "Epoch 668 | Train loss: 1.8721 | Val loss: 1.9080\n",
      "Epoch 669 | Train loss: 1.8734 | Val loss: 1.8870\n",
      "Epoch 670 | Train loss: 1.8946 | Val loss: 1.9274\n",
      "Epoch 671 | Train loss: 1.8893 | Val loss: 1.9031\n",
      "Epoch 672 | Train loss: 1.8761 | Val loss: 1.8796\n",
      "Epoch 673 | Train loss: 1.8946 | Val loss: 1.8803\n",
      "Epoch 674 | Train loss: 1.8608 | Val loss: 1.9083\n",
      "Epoch 675 | Train loss: 1.8893 | Val loss: 1.8895\n",
      "Epoch 676 | Train loss: 1.8796 | Val loss: 1.9106\n",
      "Epoch 677 | Train loss: 1.8816 | Val loss: 1.8939\n",
      "Epoch 678 | Train loss: 1.9241 | Val loss: 1.9395\n",
      "Epoch 679 | Train loss: 1.8869 | Val loss: 1.8793\n",
      "Epoch 680 | Train loss: 1.8690 | Val loss: 1.9063\n",
      "Epoch 681 | Train loss: 1.8647 | Val loss: 1.8803\n",
      "Epoch 682 | Train loss: 1.8860 | Val loss: 1.8730\n",
      "Epoch 683 | Train loss: 1.8839 | Val loss: 1.8984\n",
      "Epoch 684 | Train loss: 1.9089 | Val loss: 1.8814\n",
      "Epoch 685 | Train loss: 1.8693 | Val loss: 1.9001\n",
      "Epoch 686 | Train loss: 1.8817 | Val loss: 1.9138\n",
      "Epoch 687 | Train loss: 1.8689 | Val loss: 1.9093\n",
      "Epoch 688 | Train loss: 1.8647 | Val loss: 1.8845\n",
      "Epoch 689 | Train loss: 1.8691 | Val loss: 1.8821\n",
      "Epoch 690 | Train loss: 1.8836 | Val loss: 1.8941\n",
      "Epoch 691 | Train loss: 1.8931 | Val loss: 1.8704\n",
      "Epoch 692 | Train loss: 1.8952 | Val loss: 1.9330\n",
      "Epoch 693 | Train loss: 1.8676 | Val loss: 1.8782\n",
      "Epoch 694 | Train loss: 1.8462 | Val loss: 1.9082\n",
      "Epoch 695 | Train loss: 1.8779 | Val loss: 1.8952\n",
      "Epoch 696 | Train loss: 1.8703 | Val loss: 1.9067\n",
      "Epoch 697 | Train loss: 1.8771 | Val loss: 1.8806\n",
      "Epoch 698 | Train loss: 1.8817 | Val loss: 1.9183\n",
      "Epoch 699 | Train loss: 1.8926 | Val loss: 1.8616\n",
      "Epoch 700 | Train loss: 1.8837 | Val loss: 1.9199\n",
      "Epoch 701 | Train loss: 1.8731 | Val loss: 1.9034\n",
      "Epoch 702 | Train loss: 1.8599 | Val loss: 1.8771\n",
      "Epoch 703 | Train loss: 1.8640 | Val loss: 1.9004\n",
      "Epoch 704 | Train loss: 1.8614 | Val loss: 1.8900\n",
      "Epoch 705 | Train loss: 1.8484 | Val loss: 1.8924\n",
      "Epoch 706 | Train loss: 1.8485 | Val loss: 1.9084\n",
      "Epoch 707 | Train loss: 1.8471 | Val loss: 1.8941\n",
      "Epoch 708 | Train loss: 1.8540 | Val loss: 1.8907\n",
      "Epoch 709 | Train loss: 1.8687 | Val loss: 1.9145\n",
      "Epoch 710 | Train loss: 1.8589 | Val loss: 1.8724\n",
      "Epoch 711 | Train loss: 1.8682 | Val loss: 1.8513\n",
      "Epoch 712 | Train loss: 1.8379 | Val loss: 1.8665\n",
      "Epoch 713 | Train loss: 1.8359 | Val loss: 1.8913\n",
      "Epoch 714 | Train loss: 1.8416 | Val loss: 1.8853\n",
      "Epoch 715 | Train loss: 1.8441 | Val loss: 1.8773\n",
      "Epoch 716 | Train loss: 1.8780 | Val loss: 1.8482\n",
      "Epoch 717 | Train loss: 1.8436 | Val loss: 1.8648\n",
      "Epoch 718 | Train loss: 1.8462 | Val loss: 1.8994\n",
      "Epoch 719 | Train loss: 1.8637 | Val loss: 1.8605\n",
      "Epoch 720 | Train loss: 1.8658 | Val loss: 1.8219\n",
      "Epoch 721 | Train loss: 1.8546 | Val loss: 1.8521\n",
      "Epoch 722 | Train loss: 1.8141 | Val loss: 1.8673\n",
      "Epoch 723 | Train loss: 1.8395 | Val loss: 1.8621\n",
      "Epoch 724 | Train loss: 1.8452 | Val loss: 1.8470\n",
      "Epoch 725 | Train loss: 1.8764 | Val loss: 1.8734\n",
      "Epoch 726 | Train loss: 1.8654 | Val loss: 1.8673\n",
      "Epoch 727 | Train loss: 1.8482 | Val loss: 1.9130\n",
      "Epoch 728 | Train loss: 1.8590 | Val loss: 1.9007\n",
      "Epoch 729 | Train loss: 1.8423 | Val loss: 1.8888\n",
      "Epoch 730 | Train loss: 1.8645 | Val loss: 1.8704\n",
      "Epoch 731 | Train loss: 1.8331 | Val loss: 1.8839\n",
      "Epoch 732 | Train loss: 1.8453 | Val loss: 1.8921\n",
      "Epoch 733 | Train loss: 1.8228 | Val loss: 1.8469\n",
      "Epoch 734 | Train loss: 1.8479 | Val loss: 1.8501\n",
      "Epoch 735 | Train loss: 1.8557 | Val loss: 1.8778\n",
      "Epoch 736 | Train loss: 1.8610 | Val loss: 1.8577\n",
      "Epoch 737 | Train loss: 1.8465 | Val loss: 1.8263\n",
      "Epoch 738 | Train loss: 1.8476 | Val loss: 1.8920\n",
      "Epoch 739 | Train loss: 1.8663 | Val loss: 1.8874\n",
      "Epoch 740 | Train loss: 1.8761 | Val loss: 1.8080\n",
      "Epoch 741 | Train loss: 1.8197 | Val loss: 1.8841\n",
      "Epoch 742 | Train loss: 1.7861 | Val loss: 1.8674\n",
      "Epoch 743 | Train loss: 1.8337 | Val loss: 1.8618\n",
      "Epoch 744 | Train loss: 1.8265 | Val loss: 1.8505\n",
      "Epoch 745 | Train loss: 1.8234 | Val loss: 1.9213\n",
      "Epoch 746 | Train loss: 1.8268 | Val loss: 1.8681\n",
      "Epoch 747 | Train loss: 1.8559 | Val loss: 1.8858\n",
      "Epoch 748 | Train loss: 1.8234 | Val loss: 1.8778\n",
      "Epoch 749 | Train loss: 1.8112 | Val loss: 1.8281\n",
      "Epoch 750 | Train loss: 1.8508 | Val loss: 1.8924\n",
      "Epoch 751 | Train loss: 1.8211 | Val loss: 1.8664\n",
      "Epoch 752 | Train loss: 1.8504 | Val loss: 1.8496\n",
      "Epoch 753 | Train loss: 1.8128 | Val loss: 1.8664\n",
      "Epoch 754 | Train loss: 1.8265 | Val loss: 1.8332\n",
      "Epoch 755 | Train loss: 1.8369 | Val loss: 1.8819\n",
      "Epoch 756 | Train loss: 1.8170 | Val loss: 1.9153\n",
      "Epoch 757 | Train loss: 1.8426 | Val loss: 1.8411\n",
      "Epoch 758 | Train loss: 1.8523 | Val loss: 1.8787\n",
      "Epoch 759 | Train loss: 1.7987 | Val loss: 1.8668\n",
      "Epoch 760 | Train loss: 1.8336 | Val loss: 1.8831\n",
      "Epoch 761 | Train loss: 1.8231 | Val loss: 1.8733\n",
      "Epoch 762 | Train loss: 1.8226 | Val loss: 1.8492\n",
      "Epoch 763 | Train loss: 1.8223 | Val loss: 1.8653\n",
      "Epoch 764 | Train loss: 1.8124 | Val loss: 1.8646\n",
      "Epoch 765 | Train loss: 1.8575 | Val loss: 1.8969\n",
      "Epoch 766 | Train loss: 1.8348 | Val loss: 1.8179\n",
      "Epoch 767 | Train loss: 1.7931 | Val loss: 1.8473\n",
      "Epoch 768 | Train loss: 1.8015 | Val loss: 1.8754\n",
      "Epoch 769 | Train loss: 1.8347 | Val loss: 1.8688\n",
      "Epoch 770 | Train loss: 1.8236 | Val loss: 1.8835\n",
      "Epoch 771 | Train loss: 1.8364 | Val loss: 1.8414\n",
      "Epoch 772 | Train loss: 1.8254 | Val loss: 1.8282\n",
      "Epoch 773 | Train loss: 1.8168 | Val loss: 1.8713\n",
      "Epoch 774 | Train loss: 1.8341 | Val loss: 1.8159\n",
      "Epoch 775 | Train loss: 1.8304 | Val loss: 1.8203\n",
      "Epoch 776 | Train loss: 1.8386 | Val loss: 1.8388\n",
      "Epoch 777 | Train loss: 1.8235 | Val loss: 1.8344\n",
      "Epoch 778 | Train loss: 1.8275 | Val loss: 1.8721\n",
      "Epoch 779 | Train loss: 1.8218 | Val loss: 1.8239\n",
      "Epoch 780 | Train loss: 1.8081 | Val loss: 1.8393\n",
      "Epoch 781 | Train loss: 1.7802 | Val loss: 1.8436\n",
      "Epoch 782 | Train loss: 1.8041 | Val loss: 1.8615\n",
      "Epoch 783 | Train loss: 1.8233 | Val loss: 1.8867\n",
      "Epoch 784 | Train loss: 1.8154 | Val loss: 1.8608\n",
      "Epoch 785 | Train loss: 1.8226 | Val loss: 1.8390\n",
      "Epoch 786 | Train loss: 1.8108 | Val loss: 1.8890\n",
      "Epoch 787 | Train loss: 1.8069 | Val loss: 1.8371\n",
      "Epoch 788 | Train loss: 1.8141 | Val loss: 1.8339\n",
      "Epoch 789 | Train loss: 1.8032 | Val loss: 1.8687\n",
      "Epoch 790 | Train loss: 1.7993 | Val loss: 1.8756\n",
      "Epoch 791 | Train loss: 1.8060 | Val loss: 1.8379\n",
      "Epoch 792 | Train loss: 1.7843 | Val loss: 1.8489\n",
      "Epoch 793 | Train loss: 1.8073 | Val loss: 1.8342\n",
      "Epoch 794 | Train loss: 1.8064 | Val loss: 1.8608\n",
      "Epoch 795 | Train loss: 1.8149 | Val loss: 1.8717\n",
      "Epoch 796 | Train loss: 1.7952 | Val loss: 1.8610\n",
      "Epoch 797 | Train loss: 1.7530 | Val loss: 1.8747\n",
      "Epoch 798 | Train loss: 1.8044 | Val loss: 1.8336\n",
      "Epoch 799 | Train loss: 1.8132 | Val loss: 1.8492\n",
      "Epoch 800 | Train loss: 1.7660 | Val loss: 1.8676\n",
      "Epoch 801 | Train loss: 1.8197 | Val loss: 1.8458\n",
      "Epoch 802 | Train loss: 1.7995 | Val loss: 1.8462\n",
      "Epoch 803 | Train loss: 1.7795 | Val loss: 1.8294\n",
      "Epoch 804 | Train loss: 1.7796 | Val loss: 1.8575\n",
      "Epoch 805 | Train loss: 1.7877 | Val loss: 1.8211\n",
      "Epoch 806 | Train loss: 1.8054 | Val loss: 1.8368\n",
      "Epoch 807 | Train loss: 1.7794 | Val loss: 1.8344\n",
      "Epoch 808 | Train loss: 1.7990 | Val loss: 1.8427\n",
      "Epoch 809 | Train loss: 1.7975 | Val loss: 1.8178\n",
      "Epoch 810 | Train loss: 1.7627 | Val loss: 1.8498\n",
      "Epoch 811 | Train loss: 1.7914 | Val loss: 1.8574\n",
      "Epoch 812 | Train loss: 1.7680 | Val loss: 1.8098\n",
      "Epoch 813 | Train loss: 1.7850 | Val loss: 1.8472\n",
      "Epoch 814 | Train loss: 1.8343 | Val loss: 1.8325\n",
      "Epoch 815 | Train loss: 1.7925 | Val loss: 1.8331\n",
      "Epoch 816 | Train loss: 1.7625 | Val loss: 1.8550\n",
      "Epoch 817 | Train loss: 1.7944 | Val loss: 1.8358\n",
      "Epoch 818 | Train loss: 1.7967 | Val loss: 1.8298\n",
      "Epoch 819 | Train loss: 1.8204 | Val loss: 1.8249\n",
      "Epoch 820 | Train loss: 1.7630 | Val loss: 1.8226\n",
      "Epoch 821 | Train loss: 1.7754 | Val loss: 1.8402\n",
      "Epoch 822 | Train loss: 1.7985 | Val loss: 1.8303\n",
      "Epoch 823 | Train loss: 1.7911 | Val loss: 1.8429\n",
      "Epoch 824 | Train loss: 1.8116 | Val loss: 1.8245\n",
      "Epoch 825 | Train loss: 1.7888 | Val loss: 1.8646\n",
      "Epoch 826 | Train loss: 1.7574 | Val loss: 1.8702\n",
      "Epoch 827 | Train loss: 1.7958 | Val loss: 1.8363\n",
      "Epoch 828 | Train loss: 1.7901 | Val loss: 1.8416\n",
      "Epoch 829 | Train loss: 1.8087 | Val loss: 1.8456\n",
      "Epoch 830 | Train loss: 1.7751 | Val loss: 1.8129\n",
      "Epoch 831 | Train loss: 1.7738 | Val loss: 1.8634\n",
      "Epoch 832 | Train loss: 1.7915 | Val loss: 1.9077\n",
      "Epoch 833 | Train loss: 1.8050 | Val loss: 1.8271\n",
      "Epoch 834 | Train loss: 1.7907 | Val loss: 1.8560\n",
      "Epoch 835 | Train loss: 1.7837 | Val loss: 1.8200\n",
      "Epoch 836 | Train loss: 1.8211 | Val loss: 1.8082\n",
      "Epoch 837 | Train loss: 1.7462 | Val loss: 1.8261\n",
      "Epoch 838 | Train loss: 1.8064 | Val loss: 1.8730\n",
      "Epoch 839 | Train loss: 1.7924 | Val loss: 1.7973\n",
      "Epoch 840 | Train loss: 1.7808 | Val loss: 1.8341\n",
      "Epoch 841 | Train loss: 1.7775 | Val loss: 1.8356\n",
      "Epoch 842 | Train loss: 1.7603 | Val loss: 1.8600\n",
      "Epoch 843 | Train loss: 1.7695 | Val loss: 1.8475\n",
      "Epoch 844 | Train loss: 1.7729 | Val loss: 1.8502\n",
      "Epoch 845 | Train loss: 1.7665 | Val loss: 1.8642\n",
      "Epoch 846 | Train loss: 1.7364 | Val loss: 1.8403\n",
      "Epoch 847 | Train loss: 1.7538 | Val loss: 1.8409\n",
      "Epoch 848 | Train loss: 1.7285 | Val loss: 1.8040\n",
      "Epoch 849 | Train loss: 1.7923 | Val loss: 1.8629\n",
      "Epoch 850 | Train loss: 1.7633 | Val loss: 1.8425\n",
      "Epoch 851 | Train loss: 1.7683 | Val loss: 1.8196\n",
      "Epoch 852 | Train loss: 1.7651 | Val loss: 1.8487\n",
      "Epoch 853 | Train loss: 1.7415 | Val loss: 1.7876\n",
      "Epoch 854 | Train loss: 1.7750 | Val loss: 1.8633\n",
      "Epoch 855 | Train loss: 1.7675 | Val loss: 1.8543\n",
      "Epoch 856 | Train loss: 1.7906 | Val loss: 1.7743\n",
      "Epoch 857 | Train loss: 1.7458 | Val loss: 1.8695\n",
      "Epoch 858 | Train loss: 1.7333 | Val loss: 1.8197\n",
      "Epoch 859 | Train loss: 1.7902 | Val loss: 1.8443\n",
      "Epoch 860 | Train loss: 1.7393 | Val loss: 1.8154\n",
      "Epoch 861 | Train loss: 1.7599 | Val loss: 1.8195\n",
      "Epoch 862 | Train loss: 1.7841 | Val loss: 1.8185\n",
      "Epoch 863 | Train loss: 1.7819 | Val loss: 1.8130\n",
      "Epoch 864 | Train loss: 1.7755 | Val loss: 1.8073\n",
      "Epoch 865 | Train loss: 1.7352 | Val loss: 1.8012\n",
      "Epoch 866 | Train loss: 1.7821 | Val loss: 1.8611\n",
      "Epoch 867 | Train loss: 1.7491 | Val loss: 1.8347\n",
      "Epoch 868 | Train loss: 1.7492 | Val loss: 1.8083\n",
      "Epoch 869 | Train loss: 1.7542 | Val loss: 1.7930\n",
      "Epoch 870 | Train loss: 1.7490 | Val loss: 1.7966\n",
      "Epoch 871 | Train loss: 1.7528 | Val loss: 1.8582\n",
      "Epoch 872 | Train loss: 1.7337 | Val loss: 1.8380\n",
      "Epoch 873 | Train loss: 1.7534 | Val loss: 1.7901\n",
      "Epoch 874 | Train loss: 1.7796 | Val loss: 1.7897\n",
      "Epoch 875 | Train loss: 1.7579 | Val loss: 1.7817\n",
      "Epoch 876 | Train loss: 1.7831 | Val loss: 1.7932\n",
      "Epoch 877 | Train loss: 1.7440 | Val loss: 1.7943\n",
      "Epoch 878 | Train loss: 1.7410 | Val loss: 1.8147\n",
      "Epoch 879 | Train loss: 1.7614 | Val loss: 1.8363\n",
      "Epoch 880 | Train loss: 1.7635 | Val loss: 1.7828\n",
      "Epoch 881 | Train loss: 1.7271 | Val loss: 1.8406\n",
      "Epoch 882 | Train loss: 1.7556 | Val loss: 1.8182\n",
      "Epoch 883 | Train loss: 1.7642 | Val loss: 1.7653\n",
      "Epoch 884 | Train loss: 1.7629 | Val loss: 1.7683\n",
      "Epoch 885 | Train loss: 1.7199 | Val loss: 1.7852\n",
      "Epoch 886 | Train loss: 1.7681 | Val loss: 1.8545\n",
      "Epoch 887 | Train loss: 1.7421 | Val loss: 1.8232\n",
      "Epoch 888 | Train loss: 1.7746 | Val loss: 1.8301\n",
      "Epoch 889 | Train loss: 1.7615 | Val loss: 1.7894\n",
      "Epoch 890 | Train loss: 1.7656 | Val loss: 1.7898\n",
      "Epoch 891 | Train loss: 1.7266 | Val loss: 1.7855\n",
      "Epoch 892 | Train loss: 1.7506 | Val loss: 1.8182\n",
      "Epoch 893 | Train loss: 1.7310 | Val loss: 1.8282\n",
      "Epoch 894 | Train loss: 1.7577 | Val loss: 1.7869\n",
      "Epoch 895 | Train loss: 1.7215 | Val loss: 1.7956\n",
      "Epoch 896 | Train loss: 1.7473 | Val loss: 1.8200\n",
      "Epoch 897 | Train loss: 1.7557 | Val loss: 1.7609\n",
      "Epoch 898 | Train loss: 1.7662 | Val loss: 1.7780\n",
      "Epoch 899 | Train loss: 1.7544 | Val loss: 1.8322\n",
      "Epoch 900 | Train loss: 1.7439 | Val loss: 1.7995\n",
      "Epoch 901 | Train loss: 1.7674 | Val loss: 1.7567\n",
      "Epoch 902 | Train loss: 1.7518 | Val loss: 1.8063\n",
      "Epoch 903 | Train loss: 1.7738 | Val loss: 1.8029\n",
      "Epoch 904 | Train loss: 1.7304 | Val loss: 1.7818\n",
      "Epoch 905 | Train loss: 1.7599 | Val loss: 1.8305\n",
      "Epoch 906 | Train loss: 1.7340 | Val loss: 1.7680\n",
      "Epoch 907 | Train loss: 1.7428 | Val loss: 1.8371\n",
      "Epoch 908 | Train loss: 1.7391 | Val loss: 1.8245\n",
      "Epoch 909 | Train loss: 1.7633 | Val loss: 1.7893\n",
      "Epoch 910 | Train loss: 1.7467 | Val loss: 1.7844\n",
      "Epoch 911 | Train loss: 1.7337 | Val loss: 1.7806\n",
      "Epoch 912 | Train loss: 1.7466 | Val loss: 1.8003\n",
      "Epoch 913 | Train loss: 1.7084 | Val loss: 1.7935\n",
      "Epoch 914 | Train loss: 1.7437 | Val loss: 1.8071\n",
      "Epoch 915 | Train loss: 1.7360 | Val loss: 1.7815\n",
      "Epoch 916 | Train loss: 1.7501 | Val loss: 1.7882\n",
      "Epoch 917 | Train loss: 1.7152 | Val loss: 1.7795\n",
      "Epoch 918 | Train loss: 1.7750 | Val loss: 1.8147\n",
      "Epoch 919 | Train loss: 1.7263 | Val loss: 1.7934\n",
      "Epoch 920 | Train loss: 1.7343 | Val loss: 1.7715\n",
      "Epoch 921 | Train loss: 1.7070 | Val loss: 1.7594\n",
      "Epoch 922 | Train loss: 1.7134 | Val loss: 1.7828\n",
      "Epoch 923 | Train loss: 1.6925 | Val loss: 1.7865\n",
      "Epoch 924 | Train loss: 1.7558 | Val loss: 1.8000\n",
      "Epoch 925 | Train loss: 1.7457 | Val loss: 1.7993\n",
      "Epoch 926 | Train loss: 1.7433 | Val loss: 1.7653\n",
      "Epoch 927 | Train loss: 1.7399 | Val loss: 1.8460\n",
      "Epoch 928 | Train loss: 1.7228 | Val loss: 1.7886\n",
      "Epoch 929 | Train loss: 1.7396 | Val loss: 1.8021\n",
      "Epoch 930 | Train loss: 1.7578 | Val loss: 1.7686\n",
      "Epoch 931 | Train loss: 1.7298 | Val loss: 1.7929\n",
      "Epoch 932 | Train loss: 1.7171 | Val loss: 1.7951\n",
      "Epoch 933 | Train loss: 1.7229 | Val loss: 1.7912\n",
      "Epoch 934 | Train loss: 1.7466 | Val loss: 1.7903\n",
      "Epoch 935 | Train loss: 1.7282 | Val loss: 1.8014\n",
      "Epoch 936 | Train loss: 1.7318 | Val loss: 1.7519\n",
      "Epoch 937 | Train loss: 1.7438 | Val loss: 1.7156\n",
      "Epoch 938 | Train loss: 1.7360 | Val loss: 1.8304\n",
      "Epoch 939 | Train loss: 1.7210 | Val loss: 1.7840\n",
      "Epoch 940 | Train loss: 1.6909 | Val loss: 1.7986\n",
      "Epoch 941 | Train loss: 1.7485 | Val loss: 1.7731\n",
      "Epoch 942 | Train loss: 1.7330 | Val loss: 1.7616\n",
      "Epoch 943 | Train loss: 1.7458 | Val loss: 1.7690\n",
      "Epoch 944 | Train loss: 1.6992 | Val loss: 1.8106\n",
      "Epoch 945 | Train loss: 1.7352 | Val loss: 1.7537\n",
      "Epoch 946 | Train loss: 1.7545 | Val loss: 1.8280\n",
      "Epoch 947 | Train loss: 1.7045 | Val loss: 1.7936\n",
      "Epoch 948 | Train loss: 1.7564 | Val loss: 1.7785\n",
      "Epoch 949 | Train loss: 1.7275 | Val loss: 1.8024\n",
      "Epoch 950 | Train loss: 1.7180 | Val loss: 1.7999\n",
      "Epoch 951 | Train loss: 1.7391 | Val loss: 1.7944\n",
      "Epoch 952 | Train loss: 1.7386 | Val loss: 1.7842\n",
      "Epoch 953 | Train loss: 1.7176 | Val loss: 1.7873\n",
      "Epoch 954 | Train loss: 1.7307 | Val loss: 1.7766\n",
      "Epoch 955 | Train loss: 1.7304 | Val loss: 1.7815\n",
      "Epoch 956 | Train loss: 1.7356 | Val loss: 1.8259\n",
      "Epoch 957 | Train loss: 1.7305 | Val loss: 1.7788\n",
      "Epoch 958 | Train loss: 1.7050 | Val loss: 1.8332\n",
      "Epoch 959 | Train loss: 1.7457 | Val loss: 1.8050\n",
      "Epoch 960 | Train loss: 1.6943 | Val loss: 1.7950\n",
      "Epoch 961 | Train loss: 1.7230 | Val loss: 1.7530\n",
      "Epoch 962 | Train loss: 1.7041 | Val loss: 1.7577\n",
      "Epoch 963 | Train loss: 1.7257 | Val loss: 1.7972\n",
      "Epoch 964 | Train loss: 1.6758 | Val loss: 1.7821\n",
      "Epoch 965 | Train loss: 1.6972 | Val loss: 1.7724\n",
      "Epoch 966 | Train loss: 1.6907 | Val loss: 1.7721\n",
      "Epoch 967 | Train loss: 1.7258 | Val loss: 1.7731\n",
      "Epoch 968 | Train loss: 1.7249 | Val loss: 1.7681\n",
      "Epoch 969 | Train loss: 1.7283 | Val loss: 1.8191\n",
      "Epoch 970 | Train loss: 1.7212 | Val loss: 1.7639\n",
      "Epoch 971 | Train loss: 1.6882 | Val loss: 1.7577\n",
      "Epoch 972 | Train loss: 1.7049 | Val loss: 1.7862\n",
      "Epoch 973 | Train loss: 1.6952 | Val loss: 1.7552\n",
      "Epoch 974 | Train loss: 1.6998 | Val loss: 1.7446\n",
      "Epoch 975 | Train loss: 1.7009 | Val loss: 1.7858\n",
      "Epoch 976 | Train loss: 1.7421 | Val loss: 1.7790\n",
      "Epoch 977 | Train loss: 1.7063 | Val loss: 1.7508\n",
      "Epoch 978 | Train loss: 1.7118 | Val loss: 1.7920\n",
      "Epoch 979 | Train loss: 1.6777 | Val loss: 1.7738\n",
      "Epoch 980 | Train loss: 1.7022 | Val loss: 1.8080\n",
      "Epoch 981 | Train loss: 1.7032 | Val loss: 1.7698\n",
      "Epoch 982 | Train loss: 1.6882 | Val loss: 1.7569\n",
      "Epoch 983 | Train loss: 1.7158 | Val loss: 1.7548\n",
      "Epoch 984 | Train loss: 1.7221 | Val loss: 1.7727\n",
      "Epoch 985 | Train loss: 1.7030 | Val loss: 1.7583\n",
      "Epoch 986 | Train loss: 1.7053 | Val loss: 1.7660\n",
      "Epoch 987 | Train loss: 1.7010 | Val loss: 1.7945\n",
      "Epoch 988 | Train loss: 1.6914 | Val loss: 1.8071\n",
      "Epoch 989 | Train loss: 1.6762 | Val loss: 1.7819\n",
      "Epoch 990 | Train loss: 1.7337 | Val loss: 1.7869\n",
      "Epoch 991 | Train loss: 1.7070 | Val loss: 1.7825\n",
      "Epoch 992 | Train loss: 1.6750 | Val loss: 1.7681\n",
      "Epoch 993 | Train loss: 1.7219 | Val loss: 1.7675\n",
      "Epoch 994 | Train loss: 1.6792 | Val loss: 1.8109\n",
      "Epoch 995 | Train loss: 1.6975 | Val loss: 1.7950\n",
      "Epoch 996 | Train loss: 1.6941 | Val loss: 1.8151\n",
      "Epoch 997 | Train loss: 1.7060 | Val loss: 1.7310\n",
      "Epoch 998 | Train loss: 1.6813 | Val loss: 1.7373\n",
      "Epoch 999 | Train loss: 1.7108 | Val loss: 1.7829\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "model = Transformer_Decoder(vocab_size, num_layers, num_heads, embed_dim, max_input_length, input_dropout).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  x, y = get_batch('train', device=device, input_length=max_input_length, batch_size=batch_size)\n",
    "  y_logits = model(x)\n",
    "    \n",
    "  B, T, C = y_logits.shape\n",
    "  loss = loss_fn(y_logits.reshape(B * T, C), y.reshape(B * T))\n",
    "  train_loss += loss.item()\n",
    "\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if epoch % eval_interval == 0 or epoch == epochs-1:\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      x, y = get_batch('val', device=device, input_length=max_input_length, batch_size=batch_size)\n",
    "\n",
    "      y_logits = model(x)\n",
    "    \n",
    "      B, T, C = y_logits.shape\n",
    "      loss = loss_fn(y_logits.reshape(B * T, C), y.reshape(B * T))\n",
    "      val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")\n",
    "    \n",
    "    torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': train_loss,\n",
    "    }, 'model/shakespeare_char_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddebbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/shakespeare_char_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de459d",
   "metadata": {},
   "source": [
    "## Generate Shakespeare-like text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a860883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), torch.Size([1, 257]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer_Decoder(vocab_size, num_layers, num_heads, embed_dim, max_input_length, input_dropout).to(device)\n",
    "model.load_state_dict(torch.load('model/shakespeare_char_model_weights.pth'))\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "output = model.generate(context, vocab_size, max_new_tokens=256)\n",
    "context.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccc87715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TORWAy, think, we be the wayn tis they\n",
      "Are eye, I no she; to me rast be gounes\n",
      "All reart wilto city to farthed?\n",
      "\n",
      "GLOUCESTER:\n",
      "For thour's laster.\n",
      "\n",
      "SIR Engman:\n",
      "They arvelly deveren of and the desepterous.\n",
      "Pwhom: with thou do cemion the.\n",
      "\n",
      "GLOUCESTER:\n",
      "By ples \n"
     ]
    }
   ],
   "source": [
    "print(decode(output[0].cpu().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
